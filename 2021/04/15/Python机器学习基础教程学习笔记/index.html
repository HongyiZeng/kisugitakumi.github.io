<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Python机器学习基础教程学习笔记 | Kisugi Takumi</title><meta name="keywords" content="机器学习,Python"><meta name="author" content="Kisugi Takumi,zenghongyi1@google.com"><meta name="copyright" content="Kisugi Takumi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。">
<meta property="og:type" content="article">
<meta property="og:title" content="Python机器学习基础教程学习笔记">
<meta property="og:url" content="https://kisugitakumi.github.io/2021/04/15/Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Kisugi Takumi">
<meta property="og:description" content="机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kisugitakumi.github.io/linear-gradient(20deg,%20#0062be,%20#925696,%20#cc426e,%20#fb0347)">
<meta property="article:published_time" content="2021-04-15T03:46:25.000Z">
<meta property="article:modified_time" content="2022-04-14T12:41:52.161Z">
<meta property="article:author" content="Kisugi Takumi">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kisugitakumi.github.io/linear-gradient(20deg,%20#0062be,%20#925696,%20#cc426e,%20#fb0347)"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kisugitakumi.github.io/2021/04/15/Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Kisugi Takumi","link":"链接: ","source":"来源: Kisugi Takumi","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Python机器学习基础教程学习笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-04-14 20:41:52'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/favicon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">80</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">60</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 目录</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-brain"></i><span> 我的豆瓣</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fab fa-steam-square"></i><span> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/create/"><i class="fa-fw fas fa-user-edit"></i><span> 我的创作</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-kiss-wink-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background: linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Kisugi Takumi</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 目录</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-brain"></i><span> 我的豆瓣</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fab fa-steam-square"></i><span> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/create/"><i class="fa-fw fas fa-user-edit"></i><span> 我的创作</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-kiss-wink-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Python机器学习基础教程学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-04-15T03:46:25.000Z" title="发表于 2021-04-15 11:46:25">2021-04-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-04-14T12:41:52.161Z" title="更新于 2022-04-14 20:41:52">2022-04-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">25.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>98分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Python机器学习基础教程学习笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Python机器学习基础教程学习笔记"><a href="#Python机器学习基础教程学习笔记" class="headerlink" title="Python机器学习基础教程学习笔记"></a>Python机器学习基础教程学习笔记</h1><p>笔记学习来源：Python机器学习基础教程</p>
<p>学习时间：2021年4月15日开始学习</p>
<h2 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h2><h3 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h3><blockquote>
<p>监督学习</p>
</blockquote>
<p>从输入输出对中进行学习的算法。</p>
<p>示例：识别信封上手写的邮政编码，基于医学影像判断肿瘤是否为良性，检测信用卡交易中的诈骗行为。</p>
<blockquote>
<p>无监督学习</p>
</blockquote>
<p>只有输入数据是已知的，没有为算法提供输出数据。</p>
<p>示例：确定一系列博客文章的主题，将客户分成具有相似偏好的群组，检测网站的异常访问模式。</p>
<h3 id="1-2-环境搭建和工具介绍"><a href="#1-2-环境搭建和工具介绍" class="headerlink" title="1.2 环境搭建和工具介绍"></a>1.2 环境搭建和工具介绍</h3><blockquote>
<p><code>scikit-learn</code></p>
</blockquote>
<p>Scikit-learn（以前称为scikits.learn，也称为sklearn）是针对Python 编程语言的免费软件机器学习库。它具有各种分类，回归和聚类算法，包括支持向量机，随机森林，梯度提升，k均值和DBSCAN，并且旨在与Python数值科学图书馆NumPy和SciPy。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install numpy scipy matplotlib ipython scikit-learn pandas</span><br></pre></td></tr></table></figure>
<blockquote>
<p>必要的库和工具</p>
</blockquote>
<ul>
<li><code>NumPy</code></li>
</ul>
<p>NumPy(Numerical Python) 是 Python 语言的一个扩展程序库，支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。NumPy 是一个运行速度非常快的数学库，主要用于数组计算，核心功能是<code>ndarray</code>类，即n维数组，数组中的所有元素必须是同一类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 创建一个 ndarray 只需调用 NumPy 的 array 函数即可</span></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415145430297.png" alt="image-20210415145430297"></p>
<ul>
<li><code>SciPy</code></li>
</ul>
<p>Scipy是一个用于数学、科学、工程领域的常用软件包，可以处理插值、积分、优化、图像处理、常微分方程数值解的求解、信号处理等问题。它用于有效计算Numpy矩阵，使Numpy和Scipy协同工作，高效解决问题。最重要的是<code>scipy.sparse</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line"><span class="comment"># 创建一个二维数组NumPy，对角线为1，其余都为0</span></span><br><span class="line"><span class="comment"># eye是scipy包中的一个创建特殊矩阵(单位矩阵E)的方法</span></span><br><span class="line">eye = np.eye(<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy array:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(eye))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415145815904.png" alt="image-20210415145815904"></p>
<ul>
<li><code>matplotlib</code></li>
</ul>
<p>Matplotlib 是 Python 的绘图库。 它可与 NumPy 一起使用，提供了一种有效的 MatLab 开源替代方案。 它也可以和图形工具包一起使用，如 PyQt 和 wxPython。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 在-10和10之间生成一个数列，共100个数</span></span><br><span class="line">x = np.linspace(-<span class="number">10</span>,<span class="number">10</span>,<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 用正弦函数创建另一个数组</span></span><br><span class="line">y = np.sin(x)</span><br><span class="line"><span class="comment"># plot函数绘制一个数组关于另一个数组的折线图</span></span><br><span class="line">plt.plot(x,y,marker=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415150214522.png" alt="image-20210415150214522"></p>
<ul>
<li><code>pandas</code></li>
</ul>
<p>pandas，python+data+analysis的组合缩写，是python中基于numpy和matplotlib的第三方数据分析库，与后两者共同构成了python数据分析的基础工具包，享有数分三剑客之名。正因为pandas是在numpy基础上实现，其核心数据结构与numpy的ndarray十分相似，但pandas与numpy的关系不是替代，而是互为补充。</p>
<blockquote>
<p>默认导入环境</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br></pre></td></tr></table></figure>
<h3 id="1-3-第一个应用：鸢尾花分类"><a href="#1-3-第一个应用：鸢尾花分类" class="headerlink" title="1.3 第一个应用：鸢尾花分类"></a>1.3 第一个应用：鸢尾花分类</h3><ul>
<li>目标：构建一个机器学习模型，可以从这些已知品种的鸢尾花测量数据中进行学习，从而能够预测新鸢尾花的品种，这一模型为监督学习。</li>
<li>核心术语：单个<code>数据点</code>（一朵鸢尾花）的预期输出是这朵花的品种，对于一个数据点来说，他的品种叫做<code>标签</code>。</li>
</ul>
<h4 id="1-3-1-初识数据"><a href="#1-3-1-初识数据" class="headerlink" title="1.3.1 初识数据"></a>1.3.1 初识数据</h4><blockquote>
<p>iris数据集</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 经典数据集，可以调用load_iris()函数来加载数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris_dataset = load_iris()</span><br></pre></td></tr></table></figure>
<p>iris_dataset对象是一个Bunch对象，里面包括键和值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Keys of iris_dataset:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset.keys()))</span><br><span class="line"><span class="comment"># 注：format()函数</span></span><br><span class="line"><span class="comment"># 一种格式化字符串的函数 str.format()，它增强了字符串格式化的功能。基本语法是通过&#123;&#125;和:来代替以前的%</span></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415154827716.png" alt="image-20210415154827716"></p>
<blockquote>
<p>DESCR键</p>
</blockquote>
<p>DESCR键对应的值是数据集的简要说明。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(iris_dataset[<span class="string">&#x27;DESCR&#x27;</span>][:<span class="number">193</span>] + <span class="string">&quot;\n...&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415155400917.png" alt="image-20210415155400917"></p>
<blockquote>
<p>target_names键</p>
</blockquote>
<p>target_names键对应的值是一个字符串数组，里面包含要预测的花的品种。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Target names: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset[<span class="string">&#x27;target_names&#x27;</span>]))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415155544844.png" alt="image-20210415155544844"></p>
<blockquote>
<p>feature_names键</p>
</blockquote>
<p>feature_names键对应的值是一个字符串列表，对每一个特征进行了说明。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Feature names: \n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset[<span class="string">&#x27;feature_names&#x27;</span>]))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415155723911.png" alt="image-20210415155723911"></p>
<blockquote>
<p>data键</p>
</blockquote>
<p>data键对应的值是二维NumPy数组，每一行对应一朵花，列代表每朵花的四个测量数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape of data: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset[<span class="string">&#x27;data&#x27;</span>].shape))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415155959901.png" alt="image-20210415155959901"></p>
<p>可以看出，数组中包含150朵不同的花的测量数据。data数组的形状（shape）是样本数乘以特征数，这是scikit-learn的约定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First five rows of data:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset[<span class="string">&#x27;data&#x27;</span>][:<span class="number">5</span>]))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415160207996.png" alt="image-20210415160207996"></p>
<p>以上为data数组的前五行数据。</p>
<blockquote>
<p>target键</p>
</blockquote>
<p>target键对应的值是一个一维NumPy数组，包含的是测量过的每朵花的品种（标签）。品种被转换为0到2的整数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Target:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset[<span class="string">&#x27;target&#x27;</span>]))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415160427681.png" alt="image-20210415160427681"></p>
<h4 id="1-3-2-训练数据和测试数据"><a href="#1-3-2-训练数据和测试数据" class="headerlink" title="1.3.2 训练数据和测试数据"></a>1.3.2 训练数据和测试数据</h4><p>我们将数据集分成训练集（training set）和测试集（test set），前者用于构建模型，后者用于评估模型对于前所未见的新数据的泛化能力。</p>
<ul>
<li>训练数据：用于<code>构建</code>机器学习模型，又叫做训练集。</li>
<li>测试数据：用于<code>评估</code>机器学习模型，又叫做测试集，留出集。</li>
</ul>
<blockquote>
<p><code>train_test_split</code>函数</p>
</blockquote>
<p><code>train_test_split</code>函数可以将数据集打乱并进行拆分，这个函数将75%的行数据及对应标签作为训练集，剩下的25%作为测试集。通常，数据用大写的<code>X</code>表示，而标签用小写的<code>y</code>表示，<code>X</code>为输入的二维数组，<code>y</code>则为输出的一维数组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">iris_dataset = load_iris()</span><br><span class="line"><span class="comment"># X_train包含了75%的行数据，X_test包含了25%的行数据</span></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(iris_dataset[<span class="string">&#x27;data&#x27;</span>],iris_dataset[<span class="string">&#x27;target&#x27;</span>],random_state=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_train shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(y_train.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_test shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_test.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_test shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(y_test.shape))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415161708657.png" alt="image-20210415161708657"></p>
<h4 id="1-3-3-观察数据"><a href="#1-3-3-观察数据" class="headerlink" title="1.3.3 观察数据"></a>1.3.3 观察数据</h4><p>暂略</p>
<h4 id="1-3-4-构建第一个模型：k近邻算法"><a href="#1-3-4-构建第一个模型：k近邻算法" class="headerlink" title="1.3.4 构建第一个模型：k近邻算法"></a>1.3.4 构建第一个模型：k近邻算法</h4><blockquote>
<p>k近邻算法含义</p>
</blockquote>
<p>scikit-learn中有许多可用的分类算法。要对一个新的数据点做出预测，k近邻算法会在训练集中寻找与这个新数据点距离最近的数据点，然后将找到的数据点的标签赋给这个新数据点。</p>
<p>k近邻算法中的k的含义就是考虑训练集中与新数据点最近的任意k个邻居。</p>
<p>k近邻算法实在neighbors模块的<code>KNeighborsClassifier</code>类中实现的，最重要的参数就是这个k，即邻居的数量。</p>
<blockquote>
<p>代码实现</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="comment"># knn对象对算法进行了封装，既包括了用训练数据构建模型的算法，也包括对新数据点进行预测的算法</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 想要基于训练集来构建模型，需要调用fit方法，输入参数为X_train和y_train，二者都是NumPy数组</span></span><br><span class="line"><span class="comment"># 前者为训练数据，后者为相应的训练标签</span></span><br><span class="line">knn.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>
<h4 id="1-3-5-做出预测"><a href="#1-3-5-做出预测" class="headerlink" title="1.3.5 做出预测"></a>1.3.5 做出预测</h4><blockquote>
<p>新的数据点</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意X_new为二维数组，作为其输入</span></span><br><span class="line">X_new = np.array([[<span class="number">5</span>,<span class="number">2.9</span>,<span class="number">1</span>,<span class="number">0.2</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>做出预测——<code>predict</code></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用knn对象的predict方法进行预测</span></span><br><span class="line">prediction = knn.predict(X_new)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Prediction: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(prediction))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicted target name :&#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset[<span class="string">&#x27;target_names&#x27;</span>][prediction]))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415170753481.png" alt="image-20210415170753481"></p>
<h4 id="1-3-6-评估模型"><a href="#1-3-6-评估模型" class="headerlink" title="1.3.6 评估模型"></a>1.3.6 评估模型</h4><p>可以对测试数据中的每朵鸢尾花进行预测，并将预测结果与标签进行对比。</p>
<blockquote>
<p>代码实现</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">iris_dataset = load_iris()</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(iris_dataset[<span class="string">&#x27;data&#x27;</span>],iris_dataset[<span class="string">&#x27;target&#x27;</span>],random_state=<span class="number">0</span>)</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">knn.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># 调用knn对象的score方法计算测试集的精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(knn.score(X_test,y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210415172433706.png" alt="image-20210415172433706"></p>
<p>可知该模型的准确率为97%。</p>
<h4 id="1-3-7-小结"><a href="#1-3-7-小结" class="headerlink" title="1.3.7 小结"></a>1.3.7 小结</h4><p>下面汇总了整个训练和评估过程所必需的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分原始数据集为训练集和测试集</span></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(iris_dataset[<span class="string">&#x27;data&#x27;</span>],iris_dataset[<span class="string">&#x27;target&#x27;</span>],random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 构造k近邻算法模型</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 训练该模型</span></span><br><span class="line">knn.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># 调用knn对象的score方法计算测试集的精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(knn.score(X_test,y_test)))</span><br></pre></td></tr></table></figure>
<p>这个代码片段包含了应用scikit-learn中任何机器学习算法的核心代码。<code>fit</code>，<code>predict</code>和<code>score</code>方法是scikit-learn监督学习模型中最常用的接口。</p>
<h2 id="第二章-监督学习"><a href="#第二章-监督学习" class="headerlink" title="第二章 监督学习"></a>第二章 监督学习</h2><h3 id="2-1-分类与回归"><a href="#2-1-分类与回归" class="headerlink" title="2.1 分类与回归"></a>2.1 分类与回归</h3><p>监督机器学习问题主要有两种：分类（classification）和回归（regression）</p>
<blockquote>
<p>分类</p>
</blockquote>
<p>分类问题的目标是预测类别标签，可分为二分类和多分类问题。</p>
<blockquote>
<p>回归</p>
</blockquote>
<p>回归问题的目标是预测一个连续值。</p>
<p>区分分类和回归，就是问一个问题，输出是否具有某种连续性。</p>
<h3 id="2-2-泛化，过拟合和欠拟合"><a href="#2-2-泛化，过拟合和欠拟合" class="headerlink" title="2.2 泛化，过拟合和欠拟合"></a>2.2 泛化，过拟合和欠拟合</h3><blockquote>
<p>泛化 generalization</p>
</blockquote>
<p>如果一个模型能够对没见过的数据做出准确预测，我们就说它能够从训练集泛化到测试集。</p>
<blockquote>
<p>过拟合 overfitting</p>
</blockquote>
<p>如果在拟合模型时过分关注训练集的细节，得到一个在训练集上表现很好，但不能泛化到新数据上的模型，那么就存在过拟合。这种情况通常是构建了一个对现有信息（训练集）<code>过于复杂</code>的模型。越靠近训练集，越过拟合。或者，训练集和测试集的效率差异太大，也是过拟合。</p>
<blockquote>
<p>欠拟合 underfitting</p>
</blockquote>
<p>和过拟合相反，构建了过于简单的模型，没有充分考虑到训练集中的各种有效信息。或者，训练集和测试集的效率相似且很差，就是欠拟合。</p>
<blockquote>
<p>总结</p>
</blockquote>
<p>我们的模型越复杂，在训练数据上的预测结果就越好，但是，如果我们的模型过于复杂，我们开始过多关注训练集中每个单独的数据点，模型就不能很好的泛化到新数据上去。</p>
<p>经验：收集更多的数据，<code>适当</code>构建更复杂的模型，对监督学习任务往往特别有用，在现实世界中，你往往能够决定收集多少数据，这可能比模型调参更为有效，永远不要低估更多数据的力量。</p>
<h3 id="2-3-监督学习算法"><a href="#2-3-监督学习算法" class="headerlink" title="2.3 监督学习算法"></a>2.3 监督学习算法</h3><p>许多算法都有分类和回归两种模式。</p>
<h4 id="2-3-1-一些样本数据集"><a href="#2-3-1-一些样本数据集" class="headerlink" title="2.3.1 一些样本数据集"></a>2.3.1 一些样本数据集</h4><blockquote>
<p>二分类数据集——forge</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">X, y = mglearn.datasets.make_forge()</span><br><span class="line"><span class="comment"># 数据集绘图</span></span><br><span class="line">mglearn.discrete_scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], y)</span><br><span class="line">plt.legend([<span class="string">&quot;Class 0&quot;</span>, <span class="string">&quot;Class 1&quot;</span>], loc=<span class="number">4</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;First feature&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Second feature&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X.shape))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210416153203952.png" alt="image-20210416153203952"></p>
<p>图像以第一个特征为x轴，第二个特征为y轴，每个数据点对应图像中的一点。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210416153300067.png" alt="image-20210416153300067"></p>
<p>还可知，X数据集总共26个数据，2个特征。</p>
<blockquote>
<p>回归用数据集——wave</p>
</blockquote>
<p>wave数据集只有一个输入特征和一个连续的目标变量（或==响应==），后者是模型想要预测的对象（一个连续值）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X, y = mglearn.datasets.make_wave(n_samples=<span class="number">40</span>)</span><br><span class="line">plt.plot(X, y, <span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.ylim(-<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Target&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210416153737341.png" alt="image-20210416153737341"></p>
<p>图中x轴表示特征，y轴表示回归目标（对应的响应）。</p>
<blockquote>
<p>现实中的二分类数据集——cancer</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;keys of cancer: \n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(cancer.keys()))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210416154018036.png" alt="image-20210416154018036"></p>
<p>该数据集包含569个数据，每个数据有30个特征。212个被标记为恶性（malignant），357个被标记为良性（benign）。</p>
<blockquote>
<p>现实中的回归数据集——boston</p>
</blockquote>
<p>与这个数据集相关的任务是，利用犯罪率，是否临近查尔斯河，公路可达性等信息，预测20世纪70年代波士顿地区房屋价格的中位数，这个数据集包含506个数据点和13个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">boston = load_boston()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Data shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(boston.data.shape))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210416154442447.png" alt="image-20210416154442447"></p>
<h4 id="2-3-2-k近邻算法"><a href="#2-3-2-k近邻算法" class="headerlink" title="2.3.2 k近邻算法"></a>2.3.2 k近邻算法</h4><blockquote>
<p>k近邻分类算法</p>
</blockquote>
<p>分为单近邻和多近邻。可用第一章的知识分析两者的效率区别。</p>
<blockquote>
<p>分析KNeighborsClassifier</p>
</blockquote>
<p>查看决策边界，即算法对类别0和类别1的分界线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">X, y = mglearn.datasets.make_forge()</span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>,<span class="number">3</span>, figsize=(<span class="number">10</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> n_neighbors, ax <span class="keyword">in</span> <span class="built_in">zip</span>([<span class="number">1</span>,<span class="number">3</span>,<span class="number">9</span>], axes):</span><br><span class="line">    <span class="comment"># fit 方法返回对象本身，所以我们可以将实例化和拟合放在一行代码中</span></span><br><span class="line">    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X,y)</span><br><span class="line">    mglearn.plots.plot_2d_separator(clf, X, fill=<span class="literal">True</span>, eps=<span class="number">0.5</span>, ax=ax, alpha=<span class="number">.4</span>)</span><br><span class="line">    mglearn.discrete_scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], y, ax=ax)</span><br><span class="line">    ax.set_title(<span class="string">&quot;&#123;&#125; neighbor(s)&quot;</span>.<span class="built_in">format</span>(n_neighbors))</span><br><span class="line">    ax.set_xlabel(<span class="string">&quot;feature 0&quot;</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">&quot;feature 1&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>].legend(loc=<span class="number">3</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210416160626726.png" alt="image-20210416160626726"></p>
<p>从图中可以看出，使用单一邻居绘制的决策边界紧跟着训练数据，模型越复杂，随着邻居的个数越来越多，决策边界也越来越平滑，更平滑的边界对应着更简单的模型。</p>
<ul>
<li>应用到cancer数据集的结果：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=<span class="number">66</span>)</span><br><span class="line"></span><br><span class="line">training_accuracy=[]</span><br><span class="line">test_accuracy=[]</span><br><span class="line"></span><br><span class="line"><span class="comment"># neighbors取值从1到10</span></span><br><span class="line">neighbors_settings=<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n_neighbors <span class="keyword">in</span> neighbors_settings:</span><br><span class="line">    <span class="comment"># 构建模型</span></span><br><span class="line">    clf = KNeighborsClassifier(n_neighbors=n_neighbors)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    <span class="comment"># 记录训练集精度</span></span><br><span class="line">    training_accuracy.append(clf.score(X_train, y_train))</span><br><span class="line">    <span class="comment"># 记录泛化精度</span></span><br><span class="line">    test_accuracy.append(clf.score(X_test, y_test))</span><br><span class="line"></span><br><span class="line">plt.plot(neighbors_settings, training_accuracy, label=<span class="string">&quot;training accuracy&quot;</span>)</span><br><span class="line">plt.plot(neighbors_settings, test_accuracy, label=<span class="string">&quot;test accuracy&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Accuracy&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;n_neighbors&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210416161223780.png" alt="image-20210416161223780"></p>
<p>从图中可知，最佳点在邻居为6的附近，6之前的越靠近训练集，则过拟合，导致泛化能力下降，6以后的越远离训练集，则欠拟合，泛化能力也不好。</p>
<blockquote>
<p>k近邻回归算法</p>
</blockquote>
<p>单一近邻时，利用单一邻居的预测结果就是最近邻居的目标值。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210416161536580.png" alt="image-20210416161536580"></p>
<p>使用多个近邻时，预测结果为这些邻居的平均值。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210416161607389.png" alt="image-20210416161607389"></p>
<p>利用k近邻回归算法在scikit-learn的KNeighborsRegressor类中实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line">X, y=mglearn.datasets.make_wave(n_samples=<span class="number">40</span>)</span><br><span class="line"><span class="comment"># 划分训练测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test=train_test_split(X,y, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 实例化模型，邻居设定为3</span></span><br><span class="line">reg=KNeighborsRegressor(n_neighbors=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 拟合模型</span></span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set predictions:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(reg.predict(X_test)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set R^2: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(reg.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210416161917275.png" alt="image-20210416161917275"></p>
<p>可知准确率为0.83。</p>
<blockquote>
<p>分析KNeighborsRegressor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line">X, y=mglearn.datasets.make_wave(n_samples=<span class="number">40</span>)</span><br><span class="line"><span class="comment"># 划分训练测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test=train_test_split(X,y, random_state=<span class="number">0</span>)</span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>,<span class="number">3</span>, figsize=(<span class="number">15</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment">#创建1000个数据点，在-3和3之间均匀分布</span></span><br><span class="line">line=np.linspace(-<span class="number">3</span>,<span class="number">3</span>,<span class="number">1000</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> n_neighbors,ax <span class="keyword">in</span> <span class="built_in">zip</span>([<span class="number">1</span>,<span class="number">3</span>,<span class="number">9</span>], axes):</span><br><span class="line">    <span class="comment"># 利用1，3，9个邻居分别进行预测</span></span><br><span class="line">    reg=KNeighborsRegressor(n_neighbors=n_neighbors)</span><br><span class="line">    reg.fit(X_train, y_train)</span><br><span class="line">    ax.plot(line, reg.predict(line))</span><br><span class="line">    ax.plot(X_train, y_train, <span class="string">&#x27;^&#x27;</span>, c=mglearn.cm2(<span class="number">0</span>), markersize=<span class="number">8</span>)</span><br><span class="line">    ax.plot(X_test, y_test, <span class="string">&#x27;v&#x27;</span>, c=mglearn.cm2(<span class="number">1</span>), markersize=<span class="number">8</span>)</span><br><span class="line">    ax.set_title(</span><br><span class="line">        <span class="string">&quot;&#123;&#125; neighbor(s)\n train score: &#123;:.2f&#125; test score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">            n_neighbors, </span><br><span class="line">            reg.score(X_train, y_train),</span><br><span class="line">            reg.score(X_test, y_test)))</span><br><span class="line">    ax.set_xlabel(<span class="string">&quot;Feature&quot;</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">&quot;Target&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>].legend([<span class="string">&quot;Model predictions&quot;</span>, <span class="string">&quot;Training data/target&quot;</span>,</span><br><span class="line">                <span class="string">&quot;Test data/target&quot;</span>], loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210416162210898.png" alt="image-20210416162210898"> </p>
<p>从图中可知，仅使用单一邻居，训练集中的每个点对预测结果都有显著影响，预测结果的图像经过所有数据点，这导致测试结果非常不稳定，考虑更多邻居后，预测结果变得更加平滑，但对训练数据的拟合也不太好。</p>
<blockquote>
<p>k近邻算法优缺点分析</p>
</blockquote>
<ul>
<li>优点：容易理解，模型参数调节简单</li>
<li>缺点：训练集很大时，预测速度会变慢，这一算法对有很多特征的数据集往往效果不好。</li>
</ul>
<p>综上，k近邻算法由于预测速度慢且不能处理多特征的数据集，所以实践中往往不会用到，但可作为基准测试方法使用。</p>
<h4 id="2-3-3-线性模型"><a href="#2-3-3-线性模型" class="headerlink" title="2.3.3  线性模型"></a>2.3.3  线性模型</h4><p>线性模型利用输入特征的线性函数进行预测。</p>
<blockquote>
<p>用于回归的线性模型</p>
</blockquote>
<p>对于回归问题，线性模型预测的一般公示如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = w[0]*x[0] + w[1]*x[1] + ... + w[p]*x[p] +b</span><br></pre></td></tr></table></figure>
<p>x表示单个数据点的特征，w和b是学习模型的参数，y是模型的预测结果。</p>
<p>下面线性模型在wave数据集上学习的参数w和b：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> mglearn.datasets <span class="keyword">import</span> make_wave</span><br><span class="line"><span class="keyword">from</span> mglearn.plot_helpers <span class="keyword">import</span> cm2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_linear_regression_wave</span>():</span></span><br><span class="line">  	<span class="comment"># 获得wave数据集</span></span><br><span class="line">    X, y = make_wave(n_samples=<span class="number">60</span>)</span><br><span class="line">    <span class="comment"># 拆分数据集为训练集和测试集</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    line = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># 利用线性回归模型进行拟合</span></span><br><span class="line">    lr = LinearRegression().fit(X_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;w[0]: %f  b: %f&quot;</span> % (lr.coef_[<span class="number">0</span>], lr.intercept_))</span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">    plt.plot(line, lr.predict(line))</span><br><span class="line">    plt.plot(X, y, <span class="string">&#x27;o&#x27;</span>, c=cm2(<span class="number">0</span>))</span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    ax.spines[<span class="string">&#x27;left&#x27;</span>].set_position(<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">    ax.spines[<span class="string">&#x27;right&#x27;</span>].set_color(<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    ax.spines[<span class="string">&#x27;bottom&#x27;</span>].set_position(<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">    ax.spines[<span class="string">&#x27;top&#x27;</span>].set_color(<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    ax.set_ylim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    ax.legend([<span class="string">&quot;model&quot;</span>, <span class="string">&quot;training data&quot;</span>], loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">    ax.grid(<span class="literal">True</span>)</span><br><span class="line">    ax.set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    plot_linear_regression_wave()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417143438717.png" alt="image-20210417143438717"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417143514365.png" alt="image-20210417143514365"></p>
<p>有许多线性回归模型，区别在于如何从训练集中学习到参数w和b，以及如何控制模型的复杂度。</p>
<blockquote>
<p>标准线性回归（普通最小二乘法）</p>
</blockquote>
<p>标准线性回归寻找参数w和b，使得对训练集的预测值与真实的回归目标值y之间的均方误差最小，均方误差是指预测值和真实值之差的平方和除以样本数。线性回归没有额外的控制参数，因此无法控制模型的复杂度。</p>
<ul>
<li><p>w：斜率参数，或者权重，系数，保存在coef_属性中，为NumPy数组。sklearn将从训练数据中得出的值保存在以下划线结尾的属性中，与用户设置的参数区分开。</p>
</li>
<li><p>b：偏移，或者截距，保存在intercept_属性中，为浮点数。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X, y = mglearn.datasets.make_wave(n_samples=<span class="number">60</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">42</span>)</span><br><span class="line">lr = LinearRegression().fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;lr.coef_: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr.coef_))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;lr.intercept_: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr.intercept_))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417144639249.png" alt="image-20210417144639249"></p>
<p>因为wave数据集的数据只有一个特征，则coef_的元素个数为1。</p>
<p>在boston数据集上的准确率测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X, y = mglearn.datasets.load_extended_boston()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">lr = LinearRegression().fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Traning set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417145641931.png" alt="image-20210417145641931"></p>
<p>可以看出，训练集和测试集的效率差异过大，是<code>过拟合</code>的标志。</p>
<blockquote>
<p>岭回归——ridge regression</p>
</blockquote>
<p>岭回归特点：</p>
<ol>
<li>对系数w的拟合附加约束</li>
<li>L2正则化：希望w尽可能小，都接近于0，这样意味着每个特征对输出的影响应尽可能小，模型越简单，避免过拟合。用数学术语来讲，Ridge<code>惩罚</code>了系数w的L2范数或者w的欧式长度。惩罚系数（penalty）为<code>alpha</code>。</li>
<li>岭回归在linear_model.Ridge中实现。</li>
</ol>
<p>在boston数据集上的准确率测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">X, y = mglearn.datasets.load_extended_boston()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">ridge = Ridge().fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Traning set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(ridge.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(ridge.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417150004961.png" alt="image-20210417150004961"></p>
<p>可以看出，训练集上的准确率不及线性回归，但是测试集准确率高，表示泛化能力强。</p>
<p>岭回归模型通过用户设置alpha惩罚参数来在模型的简单性和训练集性能之间做出权衡。</p>
<p>==增大惩罚系数alpha，会使得系数coef_(或w)越趋近于0，正则化约束越强，模型越简单，从而降低了训练集性能，但是泛化能力可能会增强==。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ridge = Ridge(alpha = <span class="number">10</span>).fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>lasso回归</p>
</blockquote>
<p>lasso回归模型特点：</p>
<ol>
<li>对系数w的拟合附加约束</li>
<li>L1正则化：使用lasso时，某些系数刚好为0，这说明某些特征被模型完全忽略。lasso惩罚了系数向量的L1范数，即系数的绝对值之和。惩罚系数记也为alpha。</li>
<li>岭回归在linear_model.Lasso中实现。</li>
</ol>
<p>lasso在boston数据集上的表现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X, y = mglearn.datasets.load_extended_boston()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">lasso = Lasso().fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Traning set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lasso.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lasso.score(X_test, y_test)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of features userd: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(np.<span class="built_in">sum</span>(lasso.coef_ != <span class="number">0</span>)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417151338608.png" alt="image-20210417151338608"></p>
<p>可以看出，lasso在训练集和测试集上的表现都很差，说明存在欠拟合，并且只使用了4个特征，而单个数据点具有105个特征。</p>
<p>lasso默认alpha的值为1.0，为了提高两者表现，需要减小alpha的值，使其拟合一个更复杂的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 减小alpha的值，同时增加max_iter的值，增加运行迭代的次数</span></span><br><span class="line">lasso = Lasso(alpha=<span class="number">0.01</span>,max_iter=<span class="number">100000</span>).fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417151653278.png" alt="image-20210417151653278"></p>
<p>如果把alpha设置的太小，那么就会消除正则化的约束，使其变得和标准线性回归的结果一样。</p>
<p>在实践中，一般选择岭回归模型，如果考虑到模型的可解释性，即存在某几个特征的影响相比于其它的更显著，那么选择lasso回归模型。</p>
<blockquote>
<p>用于二分类的线性模型</p>
</blockquote>
<p>线性模型也可用于分类问题。有二分类公式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = w[<span class="number">0</span>]*x[<span class="number">0</span>] + w[<span class="number">1</span>]*x[<span class="number">1</span>] + ... + w[p]*x[p] + b &gt; <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>上式为预测设置了阈值（0），当函数值小于0时，则贴上标签-1，否则为+1。</p>
<p>最常见的两种线性分类算法为Logistic回归和线性支持向量机（linear support vector machine，简称线性SVM），注意前者虽然叫做“回归”，其实是用于分类算法。</p>
<p>两者特点：</p>
<ol>
<li>使用L2正则化。权衡系数记为<code>C</code>，与惩罚系数alpha的效果==相反==。</li>
<li>前者在<code>linear_model.LogisticRegression</code>中实现，后者在<code>svm.LinearSVC</code>中实现。</li>
</ol>
<p>两者在forge数据集上的决策边界可视化结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">X, y = mglearn.datasets.make_forge()</span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> model, ax <span class="keyword">in</span> <span class="built_in">zip</span>([LinearSVC(),LogisticRegression()], axes):</span><br><span class="line">  	<span class="comment"># 拟合模型</span></span><br><span class="line">    clf = model.fit(X,y)</span><br><span class="line">    mglearn.plots.plot_2d_separator(clf, X, fill=<span class="literal">False</span>, eps=<span class="number">0.5</span>, ax=ax, alpha=<span class="number">.7</span>)</span><br><span class="line">    mglearn.discrete_scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], y, ax=ax)</span><br><span class="line">    ax.set_title(<span class="string">&quot;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(clf.__class__.__name__))</span><br><span class="line">    ax.set_xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>].legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417153247677.png" alt="image-20210417153247677"></p>
<p>注意：两个模型中都有两个点的分类是错误的。</p>
<p>参数C对线性SVM的决策边界影响结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_linear_svc_regularization</span>():</span></span><br><span class="line">    X, y = make_blobs(centers=<span class="number">2</span>, random_state=<span class="number">4</span>, n_samples=<span class="number">30</span>)</span><br><span class="line">    fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># a carefully hand-designed dataset lol</span></span><br><span class="line">    y[<span class="number">7</span>] = <span class="number">0</span></span><br><span class="line">    y[<span class="number">27</span>] = <span class="number">0</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">.5</span>, X[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">.5</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">.5</span>, X[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">.5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ax, C <span class="keyword">in</span> <span class="built_in">zip</span>(axes, [<span class="number">1e-2</span>, <span class="number">10</span>, <span class="number">1e3</span>]):</span><br><span class="line">        discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y, ax=ax)</span><br><span class="line">				<span class="comment"># 对SVM设置权衡参数C</span></span><br><span class="line">        svm = LinearSVC(C=C, tol=<span class="number">0.00001</span>, dual=<span class="literal">False</span>).fit(X, y)</span><br><span class="line">        w = svm.coef_[<span class="number">0</span>]</span><br><span class="line">        a = -w[<span class="number">0</span>] / w[<span class="number">1</span>]</span><br><span class="line">        xx = np.linspace(<span class="number">6</span>, <span class="number">13</span>)</span><br><span class="line">        yy = a * xx - (svm.intercept_[<span class="number">0</span>]) / w[<span class="number">1</span>]</span><br><span class="line">        ax.plot(xx, yy, c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">        ax.set_xlim(x_min, x_max)</span><br><span class="line">        ax.set_ylim(y_min, y_max)</span><br><span class="line">        ax.set_xticks(())</span><br><span class="line">        ax.set_yticks(())</span><br><span class="line">        ax.set_title(<span class="string">&quot;C = %f&quot;</span> % C)</span><br><span class="line">    axes[<span class="number">0</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    plot_linear_svc_regularization()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417153551456.png" alt="image-20210417153551456"></p>
<p>Logistic回归在cancer数据集上的测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># 设置C为100，默认为1，则正则化约束变弱，模型更复杂</span></span><br><span class="line">logreg = LogisticRegression(C=<span class="number">100</span>).fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Traning set score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(logreg.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(logreg.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417154427555.png" alt="image-20210417154427555"></p>
<blockquote>
<p>用于多分类的线性模型</p>
</blockquote>
<p>许多线性分类模型只适用于二分类问题，将其推广到多分类算法的一种常见方法是“一对其余”。对每个类别都学习一个二分类模型，将这个类别与其所有其他类别尽量区分开，这样就生成了与类别个数相同的二分类模型。在测试点上运行所有二分类器来进行预测，在对应类别上分数最高的分类器胜出，将这个类别标签返回作为预测结果。</p>
<p>以下是一个三分类的数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">X, y = make_blobs(random_state=<span class="number">42</span>)</span><br><span class="line">mglearn.discrete_scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],y)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;Class 0&quot;</span>, <span class="string">&quot;Class 1&quot;</span>, <span class="string">&quot;Class 2&quot;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417163703497.png" alt="image-20210417163703497"></p>
<p>在这个数据集上训练一个线性SVC分类器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line">X, y = make_blobs(random_state=<span class="number">42</span>)</span><br><span class="line">linear_svm = LinearSVC().fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coeffient shape: &quot;</span>,linear_svm.coef_.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Intercept shape: &quot;</span>,linear_svm.intercept_.shape)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417164023296.png" alt="image-20210417164023296"></p>
<p>coef_的形状为（3,2），说明每行包含三个之一的系数向量，每列包含2个特征的系数值。可视化结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X, y = make_blobs(random_state=<span class="number">42</span>)</span><br><span class="line">linear_svm = LinearSVC().fit(X, y)</span><br><span class="line">mglearn.discrete_scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],y)</span><br><span class="line">line = np.linspace(-<span class="number">15</span>,<span class="number">15</span>)</span><br><span class="line"><span class="keyword">for</span> coef, intercept, color <span class="keyword">in</span> <span class="built_in">zip</span>(linear_svm.coef_, linear_svm.intercept_, [<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;r&#x27;</span>,<span class="string">&#x27;g&#x27;</span>]):</span><br><span class="line">    plt.plot(line, -(line * coef[<span class="number">0</span>] + intercept) / coef[<span class="number">1</span>], c=color)</span><br><span class="line">plt.ylim(-<span class="number">10</span>,<span class="number">15</span>)</span><br><span class="line">plt.xlim(-<span class="number">10</span>,<span class="number">8</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;Class 0&quot;</span>, <span class="string">&quot;Class 1&quot;</span>, <span class="string">&quot;Class 2&quot;</span>,<span class="string">&quot;line class 0&quot;</span>, <span class="string">&quot;line class 1&quot;</span>,<span class="string">&quot;line class 2&quot;</span>],loc=(<span class="number">1.01</span>,<span class="number">0.3</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417164716398.png" alt="image-20210417164716398"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210417165104817.png" alt="image-20210417165104817"></p>
<blockquote>
<p>总结</p>
</blockquote>
<p>线性模型的主要参数时正则化参数，或称为惩罚参数，权衡参数，在回归模型中为alpha，分类模型中为C。</p>
<ul>
<li><p>优点：训练速度快，预测速度也快，可以推广到非常大的数据集，或者特征数量很多的数据集。</p>
</li>
<li><p>缺点：在低维数据集中（特征数较少），表现不是很好。</p>
</li>
</ul>
<blockquote>
<p>补充：<code>方法链</code></p>
</blockquote>
<p>sklearn中的所有模型的<code>fit</code>方法返回的都是<code>self</code>，因此可以编写下面的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用一行代码初始化模型并拟合</span></span><br><span class="line">logreg = LogisticRegression().fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<p>这种方式被称为<code>方法链</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">logreg = LogisticRegression()</span><br><span class="line"><span class="comment"># 另一个常见的用法是在一行代码中同时实现拟合和预测</span></span><br><span class="line">y_pred = logreg.fit(X_train, y_train).predict(X_test)</span><br></pre></td></tr></table></figure>
<p>甚至可以在一行代码中实现模型初始化，拟合和预测，但不推荐，因为代码可读性差。此外，拟合后的模型也没有保存在任何变量中，形成了“浪费”。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = LogisticRegression().fit(X_train, y_train).predict(X_test)</span><br></pre></td></tr></table></figure>
<h4 id="2-3-4-朴素贝叶斯分类器"><a href="#2-3-4-朴素贝叶斯分类器" class="headerlink" title="2.3.4 朴素贝叶斯分类器"></a>2.3.4 朴素贝叶斯分类器</h4><blockquote>
<p>特点</p>
</blockquote>
<p>与线性模型分类器相似，但训练速度更快，因此泛化能力相比较弱。</p>
<blockquote>
<p>三类分类器</p>
</blockquote>
<ul>
<li><code>GaussianNB</code>：可应用于任意连续数据，高维数据，文本数据分类。</li>
<li><code>BernoulliNB</code>：它假定输入数据为二分类数据。</li>
<li><code>MultinomiaNB</code>：它假定输入数据为计数数据，例如统计单词出现次数，可用于文本数据分类。</li>
</ul>
<h4 id="2-3-5-决策树"><a href="#2-3-5-决策树" class="headerlink" title="2.3.5 决策树"></a>2.3.5 决策树</h4><p>决策树是广泛应用于分类和回归任务的模型。本质上，它从一层层的if/else问题中进行学习，并得出结论。</p>
<p>决策树的目标就是通过提出<code>尽可能少</code>的if/else问题来得到正确答案。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210425144035732.png" alt="image-20210425144035732"></p>
<p>上图，用机器语言来讲，就是为了区分四种动物，利用三个特征来构建一个模型。</p>
<blockquote>
<p>构造决策树</p>
</blockquote>
<p>数据通常并不是上例中的那样具有二元性（是或否）的形式，而是表示为连续特征。用于连续数据的测试形式为：==特征i的值是否大于a？==</p>
<p>对数据进行反复递归划分，直到划分后的每个区域（决策树的每个叶节点）只包含单一目标值（单一类别或单一回归值）。如果树中某个叶节点所包含数据点的目标值都相同，那么这个叶节点就是纯的。</p>
<p>决策树也可用于回归任务，预测的方法是：基于每个节点的测试对树进行遍历，最终找到新数据点所属的叶节点。这一数据点的输出就是此叶节点中所有训练点的平均目标值。</p>
<blockquote>
<p>控制决策树的复杂度——防止过拟合</p>
</blockquote>
<p>通常来说，构造决策树直到所有叶节点都是纯的叶节点，会导致模型非常复杂（对训练集数据的预测率将会是100%），泛化能力会很低。</p>
<p>防止过拟合的策略有两种：</p>
<ul>
<li>预剪枝（pre-pruning）：及时停止树的生长，可以限制树的最大深度，限制叶节点的最大数目，或者规定一个节点中数据点的最小数目来防止继续划分。</li>
<li>后剪枝（post-pruning）：先构造树，然后删除或折叠信息量很少的节点。</li>
</ul>
<p>sklearn的决策树在<code>DecisionTreeRegressor</code>和<code>DecisionTreeClassifier</code>类中实现，但只实现了预剪枝。</p>
<p>决策树在cancer数据集上的表现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=<span class="number">42</span>)</span><br><span class="line">tree = DecisionTreeClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">tree.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(tree.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(tree.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210425145940981.png" alt="image-20210425145940981"></p>
<p>可以看出，训练集准确率为100%，出现了过拟合。如果不限制决策树的深度，它的深度和复杂度会非常大，<code>因此未剪枝的树容易出现过拟合</code>，对新数据的泛化能力不佳。</p>
<p>可以设置参数max_depth的值，来限制树的最大深度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree = DecisionTreeClassifier(max_depth=<span class="number">4</span>,random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210425150206666.png" alt="image-20210425150206666"></p>
<p>可以看出，训练集下降了，但是测试集的准确率上升，泛化能力得到了增强。</p>
<blockquote>
<p>分析决策树——树的可视化</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rom sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line">export_graphviz(tree,out_file=<span class="string">&quot;tree.dot&quot;</span>,class_names=[<span class="string">&quot;magligant&quot;</span>,<span class="string">&quot;benign&quot;</span>],feature_names=cancer.feature_names,impurity=<span class="literal">False</span>,filled=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;tree.dot&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    dot_graph=f.read()</span><br><span class="line">graphviz.Source(dot_graph)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>树的特征重要性——<code>fearture importance</code></p>
</blockquote>
<p>特征重要性：决策树为每个特征对树的决策的重要性进行排序，是一个0到1之间的数字，其中0表示根本没有用到，1表示完美预测目标值，特征重要性之和始终为1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Feature importance: \n &#123;&#125;&quot;</span>.<span class="built_in">format</span>(tree.feature_importances_))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210425152037005.png" alt="image-20210425152037005"></p>
<p>柱状图可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=<span class="number">42</span>)</span><br><span class="line">tree = DecisionTreeClassifier(max_depth=<span class="number">4</span>,random_state=<span class="number">0</span>)</span><br><span class="line">tree.fit(X_train, y_train)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_feature_importances_cancer</span>(<span class="params">model</span>):</span></span><br><span class="line">    n_features = cancer.data.shape[<span class="number">1</span>]</span><br><span class="line">    plt.barh(<span class="built_in">range</span>(n_features), model.feature_importances_, align=<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">    plt.yticks(np.arange(n_features), cancer.feature_names)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Feature importance&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Feature&quot;</span>)</span><br><span class="line">plot_feature_importances_cancer(tree)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210425152649499.png" alt="image-20210425152649499"></p>
<p>可以看出，顶部划分用到的特征（worst radius）是最重要的特征，第一层划分已经将两个类别区分得很好。</p>
<p>特征重要性告诉我们worst radius特征最重要，但是并没有告诉我们半径大表示样本是良性还是恶性，在特征和类别之间没有这样简单的正比反比关系。</p>
<blockquote>
<p>用于回归任务的决策树</p>
</blockquote>
<p>注意：==基于树的回归模型不能外推，也不能在训练数据范围之外进行预测。==</p>
<p>示例：以下为ram历史价格的数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">ram_prices = pd.read_csv(<span class="string">&quot;data/ram_price.csv&quot;</span>)</span><br><span class="line">plt.semilogy(ram_prices.date, ram_prices.price)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Year&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Price in $/Mbyte&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210425153508602.png" alt="image-20210425153508602"></p>
<p>我们将利用线性回归模型和决策树模型和2000年以前的历史价格来预测2000年以后的价格。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">ram_prices = pd.read_csv(<span class="string">&quot;data/ram_price.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 利用历史数据策瑜2000年以后的ram价格</span></span><br><span class="line">data_train = ram_prices[ram_prices.date &lt; <span class="number">2000</span> ]</span><br><span class="line">data_test = ram_prices[ram_prices.date &gt;= <span class="number">2000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于日期来预测价格</span></span><br><span class="line">X_train = data_train.date[:,np.newaxis]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用对数变换来得到数据和目标之间更简单的关系</span></span><br><span class="line">y_train = np.log(data_train.price)</span><br><span class="line"></span><br><span class="line">tree = DecisionTreeRegressor().fit(X_train, y_train)</span><br><span class="line">linear_reg = LinearRegression().fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对所有数据进行预测</span></span><br><span class="line">X_all = ram_prices.date[:,np.newaxis]</span><br><span class="line"></span><br><span class="line">pred_tree = tree.predict(X_all)</span><br><span class="line">pred_lr = linear_reg.predict(X_all)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数变换逆运算</span></span><br><span class="line">price_tree = np.exp(pred_tree)</span><br><span class="line">price_lr = np.exp(pred_lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制对比图像</span></span><br><span class="line">plt.semilogy(data_train.date, data_train.price, label=<span class="string">&quot;Training data&quot;</span>)</span><br><span class="line">plt.semilogy(data_test.date, data_test.price, label=<span class="string">&quot;Test data&quot;</span>)</span><br><span class="line">plt.semilogy(ram_prices.date, price_tree, label=<span class="string">&quot;Tree prediction&quot;</span>)</span><br><span class="line">plt.semilogy(ram_prices.date, price_lr, label=<span class="string">&quot;linear prediction&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210425154538978.png" alt="image-20210425154538978"></p>
<p>可以看出，两个模型的差异非常明显。对于线性模型，它丢失了训练集的某些信息，但是泛化能力很强，对2000年以后的价格预测较好。对于决策树模型，在训练集的预测上，与原始数据完全一致（图中蓝线和绿线重合在了一起），但是一旦超出训练数据的范围，即对2000年以后的价格预测上，则只能持续预测最后一个已知数据点。树不能在训练数据的范围之外生成新的响应，是所有基于树的回归模型的缺点。</p>
<blockquote>
<p>决策树的优缺点</p>
</blockquote>
<ul>
<li>优点：得到的模型很容易可视化；算法完全不受数据缩放的影响，不需要数据预处理，例如归一化或者标准化。</li>
<li>缺点：即使做了预剪枝，也经常会过拟合，泛化能力很差。实践中常常使用下一节介绍的决策树集成来代替单一的决策树模型。</li>
<li>重要参数：预剪枝参数，例如<code>max_depth</code>,<code>max_leaf_nodes</code>,<code>max_samples_leaf</code>等。</li>
</ul>
<h4 id="2-3-6-决策树集成"><a href="#2-3-6-决策树集成" class="headerlink" title="2.3.6 决策树集成"></a>2.3.6 决策树集成</h4><p>集成（ensemble）是合并多个机器学习模型来构建更强大模型的方法，已证明有两种集成模型对大量分类和回归的数据集都是有效的，二者都以决策树为基础，分别是随机森林和梯度提升决策树。</p>
<blockquote>
<p>随机森林——<code>random forest</code></p>
</blockquote>
<ul>
<li>思想：每棵树的预测都可能相对较好，但可能对部分数据存在过拟合，所以可以构造许多棵树，对这些树的结果取平均值来降低过拟合。</li>
</ul>
<p>在sklearn中，随机森林在<code>RandomForestRegressor</code>和<code>RandomForestClassifier</code>中实现，首先要确定用于构造的树的个数，取决于参数<code>n_estimators</code></p>
<ul>
<li>树的随机化有两种：</li>
</ul>
<ol>
<li><p>通过选择用于构造树的数据点：首先要对数据进行自助采样（bootstrap sample），从n_samples个数据点中有放回地重复随机抽取样本。共抽取n_samples次，这样会创建一个与原数据集想用大小的数据集，但是有些数据点（大约三分之一）会被丢失，另一些则会重复。例如：创建数据集[a,b,c,d]的自助采样，一种可能的是采样是[a,a,b,b]，或者[d,c,,d,a]等等。</p>
</li>
<li><p>通过选择每次划分测试的特征：在每个决策节点处，算法随机选择特征的一个子集（特征子集），并对其中一个特征寻找最佳测试。选择特征的个数（特征子集的大小）由参数<code>max_features</code>决定。</p>
</li>
</ol>
<p>由以上两种方法，随机森林中构造每棵树的数据集都是略有不同的，由于每个节点的特征选择，每棵树中的每次划分都是基于特征的不同子集，这两种随机化方法保证随机森林中的每棵树都是不同的。</p>
<ul>
<li><p><code>max_features</code>：关键参数。如果设置<code>max_features</code>=<code>n_featrues</code>，那么每次划分都会考虑数据集的所有特征，在特征选择的过程中就没有添加随机性。如果设置为1，那么划分时将无法选择对哪个特征进行测试。<code>max_features</code>越大，森林中的树会越相似，越小则差异越大。</p>
</li>
<li><p><code>two_moons</code>数据集介绍：这个数据集有两个类别，每个类别包含50个数据点。</p>
</li>
<li><p>分析随机森林：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line">X, y = make_moons(n_samples=<span class="number">100</span>, noise=<span class="number">0.25</span>, random_state=<span class="number">3</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">forest = RandomForestClassifier(n_estimators=<span class="number">5</span>, random_state=<span class="number">2</span>)</span><br><span class="line">forest.fit(X_train, y_train)</span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>,<span class="number">3</span>,figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> i,(ax,tree) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes.ravel(), forest.estimators_)):</span><br><span class="line">    ax.set_title(<span class="string">&quot;Tree &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)</span><br><span class="line">mglearn.plots.plot_2d_separator(forest, X_train, fill=<span class="literal">True</span>, ax=axes[-<span class="number">1</span>,-<span class="number">1</span>], alpha=<span class="number">.4</span>)</span><br><span class="line">axes[-<span class="number">1</span>,-<span class="number">1</span>].set_title(<span class="string">&quot;Random Forest&quot;</span>)</span><br><span class="line">mglearn.discrete_scatter(X_train[:,<span class="number">0</span>], X_train[:,<span class="number">1</span>], y_train)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210425165223411.png" alt="image-20210425165223411"></p>
<p>可以看出，每棵树都犯了一些错误，因为这里画出的一些训练点实际上并没有包含在这些树的训练集中，原因在于自助采样。</p>
<p>在cancer数据集上的表现（n_estimators=100）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=<span class="number">0</span>)</span><br><span class="line">forest = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">2</span>)</span><br><span class="line">forest.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(forest.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(forest.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210425165701974.png" alt="image-20210425165701974"></p>
<ul>
<li>与决策树相似，随机森林也给出了特征重要性，一般来说，后者的可靠性要比单棵树的要好。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210425165926359.png" alt="image-20210425165926359"></p>
<p>此处，随机森林认为worst perimeter是信息量最大的特征。</p>
<ul>
<li>优缺点：对于维度非常高的稀疏数据（例如文本数据），随机森林表现不是很好，这种情况通常采用线性模型处理。优点是鲁棒性很好，不需要反复调节参数，也不需要数据预处理就可以给出很好的结果。</li>
<li>重要参数：</li>
</ul>
<ol>
<li><code>n_estimators</code>：总是越大越好，鲁棒性越强，不过收益是递减的，对计算机硬件的要求就越高，训练时间也越长。</li>
<li><code>max_features</code>：决定每棵树的随机性大小，最好使用默认值，为<code>sqrt(n_features)</code>；对于回归，则为<code>n_features</code>；有时调整该参数可能提高性能。</li>
<li>预剪枝参数：预剪枝参数可能影响随机森林的性能。</li>
</ol>
<blockquote>
<p>梯度提升决策树（梯度提升机）——<code>gradient boosted decision tree</code></p>
</blockquote>
<p>梯度提升采用连续的方式构造树，每棵树都视图纠正前一棵树的错误。默认情况下，梯度提升回归树没有随机化，而是用到了预强剪枝。梯度提升树通常采用深度很小（1到5之间）的树。</p>
<ul>
<li><p><code>learn_rate</code>：学习率。用于控制每棵树纠正前一棵树的错误的强度。越大，意味着做出强的修正，模型更为复杂。</p>
</li>
<li><p>在cancer数据集上的表现：（默认100棵树，最大深度3，学习率0.1）</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=<span class="number">0</span>)</span><br><span class="line">gbrt = GradientBoostingClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">gbrt.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(gbrt.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(gbrt.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210426161050813.png" alt="image-20210426161050813"></p>
<p>出现了过拟合，可以限制最大深度，或者降低学习率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gbrt = GradientBoostingClassifier(random_state=<span class="number">0</span>, max_depth=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210426161154819.png" alt="image-20210426161154819"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gbrt = GradientBoostingClassifier(random_state=<span class="number">0</span>, learning_rate=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210426161218159.png" alt="image-20210426161218159"></p>
<ul>
<li>特征重要性：</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210426161439175.png" alt="image-20210426161439175"></p>
<ul>
<li>优点：鲁棒性强；监督学习中最强大最常用的模型之一</li>
<li>缺点：需要仔细调参，训练时间也可能比较长</li>
</ul>
<h4 id="2-3-7-核支持向量机"><a href="#2-3-7-核支持向量机" class="headerlink" title="2.3.7 核支持向量机"></a>2.3.7 核支持向量机</h4><p>核支持向量机：<code>kernelized support vector machine</code></p>
<p>可用于分类和回归，分类在SVC中实现，回归在SVR中实现。</p>
<blockquote>
<p>线性模型和非线性特征</p>
</blockquote>
<p>线性模型在低维空间中可能非常受限，因为线和平面的灵活性有限。</p>
<p>对二分类blobs数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X, y = make_blobs(centers=<span class="number">4</span>, random_state=<span class="number">8</span>)</span><br><span class="line">y = y % <span class="number">2</span></span><br><span class="line">mglearn.discrete_scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],y)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210426162208420.png" alt="image-20210426162208420"></p>
<p>这样的类别并不是线性可分的，如果采用一条直线的线性模型来划分数据点，将无法给出较好的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入线性支持向量机</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line">X, y = make_blobs(centers=<span class="number">4</span>, random_state=<span class="number">8</span>)</span><br><span class="line">y = y % <span class="number">2</span></span><br><span class="line">linear_svm = LinearSVC().fit(X, y)</span><br><span class="line">mglearn.plots.plot_2d_separator(linear_svm, X)</span><br><span class="line">mglearn.discrete_scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],y)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img3/image-20210426162547187.png" alt="image-20210426162547187"></p>
<p>现在对输入特征进行扩展，例如添加第二个特征的平方作为一个新特征，将每个数据点表示为三维点。</p>
<h4 id="2-3-8-神经网络（深度学习）"><a href="#2-3-8-神经网络（深度学习）" class="headerlink" title="2.3.8 神经网络（深度学习）"></a>2.3.8 神经网络（深度学习）</h4><h2 id="第三章-无监督学习"><a href="#第三章-无监督学习" class="headerlink" title="第三章 无监督学习"></a>第三章 无监督学习</h2><p>无监督学习包括没有已知输出，没有老师指导学习算法的各种机器学习，在无监督学习中，学习算法只有输入数据，并需要从这些数据中提取知识。</p>
<h3 id="3-1-无监督学习的类型"><a href="#3-1-无监督学习的类型" class="headerlink" title="3.1 无监督学习的类型"></a>3.1 无监督学习的类型</h3><p>主要研究两种类型的无监督学习：</p>
<ul>
<li>数据集变换</li>
<li>聚类</li>
</ul>
<p>常见应用：<code>降维</code>：接受包括许多特征的数据的高维表示，并找到表示该数据的一种新方法，用较少的特征就可以概括其重要特性；<code>聚类</code>：将数据划分成不同的组，每组包括相似的物项。<code>作为监督算法的预处理步骤</code></p>
<h3 id="3-2-无监督学习的挑战"><a href="#3-2-无监督学习的挑战" class="headerlink" title="3.2 无监督学习的挑战"></a>3.2 无监督学习的挑战</h3><p>无监督学习算法常用于不包含任何标签信息的数据，评估无监督算法结果的唯一方法就是人工检查。</p>
<h3 id="3-3-预处理和缩放"><a href="#3-3-预处理和缩放" class="headerlink" title="3.3 预处理和缩放"></a>3.3 预处理和缩放</h3><p>通常来说，这是对数据的一种简单的<code>按特征</code>的缩放和移动。</p>
<h4 id="3-3-1-不同类型的预处理"><a href="#3-3-1-不同类型的预处理" class="headerlink" title="3.3.1 不同类型的预处理"></a>3.3.1 不同类型的预处理</h4><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201612681.png" alt="image-20220414201612681"></p>
<p>左图显示的是一个有两个特征的二分类数据集，第一个特征x轴位于10-15之间，第二个特征y轴大约位于1-9之间。</p>
<p>StandardScaler确保每个特征的平均值为0，方差为1，使所有特征都位于同一量级。</p>
<p>RobustScaler与上者类似，但是利用中位数和四分位数。</p>
<p>MinMaxScaler移动数据，使所有特征都刚好位于0-1之间。</p>
<p>Normalizer对每个数据点进行缩放，使得特征向量的欧式长度为1，换言之，它将一个数据点投射到半径为1的圆上（或者球面），这意味着每个数据点的缩放比例都不同。</p>
<h4 id="3-3-2-应用数据变换"><a href="#3-3-2-应用数据变换" class="headerlink" title="3.3.2 应用数据变换"></a>3.3.2 应用数据变换</h4><p>将SVM应用在cancer数据集上，并使用MinMaxScaler来预处理数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=<span class="number">1</span>)</span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line"><span class="comment"># 使用fit方法拟合缩放器scaler，并将其应用于训练数据</span></span><br><span class="line"><span class="comment"># fit方法计算每个特征的最大值和最小值</span></span><br><span class="line">scaler.fit(X_train)</span><br><span class="line"><span class="comment"># 进行实际缩放</span></span><br><span class="line"><span class="comment"># 使用transform函数，得到模型返回数据的新表示</span></span><br><span class="line"><span class="comment"># 变换数据：</span></span><br><span class="line">X_train_scaled = scaler.transform(X_train)</span><br><span class="line"><span class="comment"># 打印出缩放前后的数据集属性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;transformed shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train_scaled.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;per-feature minimum before scaling:\n &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train.<span class="built_in">min</span>(axis=<span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;per-feature maximum before scaling:\n &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train.<span class="built_in">max</span>(axis=<span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;per-feature mininum after scaling:\n &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train_scaled.<span class="built_in">min</span>(axis=<span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;per-feature mininum after scaling:\n &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train_scaled.<span class="built_in">max</span>(axis=<span class="number">0</span>)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201637140.png" alt="image-20220414201637140"></p>
<p>可以看出没有改变数据集的形状，但是所有特征的值都位于0-1之间。</p>
<p>然后对测试集进行缩放：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_test_scaled = scaler.transform(X_test)</span><br><span class="line"><span class="comment"># 打印出缩放后的测试数据集属性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;per-feature mininum after scaling:\n &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_test_scaled.<span class="built_in">min</span>(axis=<span class="number">0</span>)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;per-feature mininum after scaling:\n &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_test_scaled.<span class="built_in">max</span>(axis=<span class="number">0</span>)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201658596.png" alt="image-20220414201658596"></p>
<p>可以看出对测试集进行缩放后其最大最小值并不是0和1，这是因为，所有的缩放器总是对训练集和测试集进行相同的变换，也就是说，<strong>transform方法总是减去训练集的最小值</strong>，然后除以训练集的范围，这两个值可能与测试集中的不同。</p>
<h4 id="3-3-3-对训练数据和测试数据进行相同的缩放"><a href="#3-3-3-对训练数据和测试数据进行相同的缩放" class="headerlink" title="3.3.3 对训练数据和测试数据进行相同的缩放"></a>3.3.3 对训练数据和测试数据进行相同的缩放</h4><p>略</p>
<blockquote>
<p>快捷方式与高效的替代方法</p>
</blockquote>
<p>通常来说，你需要在某个数据集上拟合（fit）一个模型，然后再将其transform。所有具有transform方法的模型都具有fit_transform方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line"><span class="comment"># 依次调用fit和transform（使用方法链）</span></span><br><span class="line">X_scaled = scaler.fit(X).transform(X)</span><br><span class="line"><span class="comment"># 方法2</span></span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br></pre></td></tr></table></figure>
<h4 id="3-3-4-预处理对监督学习的作用"><a href="#3-3-4-预处理对监督学习的作用" class="headerlink" title="3.3.4 预处理对监督学习的作用"></a>3.3.4 预处理对监督学习的作用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=<span class="number">0</span>)</span><br><span class="line">svm = SVC(C=<span class="number">100</span>)</span><br><span class="line">svm.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set accuracy: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(svm.score(X_test, y_test)))</span><br><span class="line"><span class="comment"># 对数据进行预处理后的准确率：</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">scaler.fit(X_train)</span><br><span class="line">X_train_scaled = scaler.transform(X_train)</span><br><span class="line">X_test_scaled = scaler.transform(X_test)</span><br><span class="line">svm.fit(X_train_scaled, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Scaled test set accuracy: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(svm.score(X_test_scaled, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201711287.png" alt="image-20220414201711287"></p>
<p>可见数据经过预处理后，准确率提高了。</p>
<h3 id="3-4-降维，特征提取与流形学习"><a href="#3-4-降维，特征提取与流形学习" class="headerlink" title="3.4 降维，特征提取与流形学习"></a>3.4 降维，特征提取与流形学习</h3><p>利用无监督学习进行数据变换可能有多种目的，最常见的目的就是可视化，压缩数据，以及寻找信息量更大的数据表示以用于进一步的处理。常见算法有主成分分析，非负矩阵分解和t-SNE。</p>
<h4 id="3-4-1主成分分析PCA"><a href="#3-4-1主成分分析PCA" class="headerlink" title="3.4.1主成分分析PCA"></a>3.4.1主成分分析PCA</h4><blockquote>
<p>简介</p>
</blockquote>
<p>PCA是一种基本的数据降维技术。 PCA是一种著名的数据降维算法，它应用的条件是数据/特征之间具有明显的线性相关性，它的两个主要的指导思想是抓主要矛盾和方差即信息，它的基本应用是<strong>数据降维</strong>，以此为基础还有<strong>数据可视化</strong>、<strong>数据压缩存储</strong>、<strong>异常检测</strong>、<strong>特征匹配与距离计算等。</strong>从数学上理解，它是一种<strong>矩阵分解</strong>算法；从物理意义上理解，它是线性空间上的<strong>线性变换</strong>。</p>
<p>主成分分析：principal component analysis。是一种旋转数据集的方法，旋转后的特征在统计上不相关。在做完这种旋转后，通常是根据新特征对解释数据的重要性来选择它的一个子集。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201722970.png" alt="image-20220414201722970"></p>
<ul>
<li><p>左上图显示的是原始数据点。算法首先找到方差最大的方向，将其标记为“成分1”（component 1）。这是数据中包含信息最多的方向（或向量），沿着这个方向的特征之间<strong>最为相关</strong>。然后算法找到与第一个方向正交且包含信息最多的方向，在二维空间中只有一个，但在高维空间中则有许多个。这些方向被称为<strong>主成分</strong>，一般来说，主成分个数与原始特征相同。</p>
</li>
<li><p>右上图将其左上图旋转得来，使得成分1与x轴平行，成分2与y轴平行。在旋转之前，从数据中减去平均值，使得变换后的数据以0为中心。</p>
</li>
<li>可以通过仅保留一部分主成分来使用pca进行<strong>降维</strong>，对于左下图，保留成分1，使得数据变成1维数据集，但要注意，并不是保留了原始特征中的某一个，而是保留了主成分的某一方向。</li>
<li>右下图由左下图反向旋转并重新加回平均值得来，这种变换有时用于去除数据中的噪声影响，或将主成分中保留的那部分信息可视化。</li>
</ul>
<blockquote>
<p>原理概述</p>
</blockquote>
<p>见打印资料</p>
<blockquote>
<p>将PCA应用于cancer数据集并可视化</p>
</blockquote>
<p>PCA最常见的应用之一是将<strong>高维数据集可视化</strong>。</p>
<p>在应用PCA之前，利用StandardScaler缩放数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(cancer.data)</span><br></pre></td></tr></table></figure>
<p>首先将PCA对象实例化，然后调用fit找到主成分，然后调用transform来旋转并降维，在这个过程中默认保留所有主成分，可以指定保留的个数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="comment"># 保留数据的前两个主成分</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 对cancer数据集拟合PCA模型</span></span><br><span class="line">pca.fit(X_scaled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据变换到前两个主成分的方向上</span></span><br><span class="line">X_pca = pca.transform(X_scaled)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Original shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(X_scaled.shape)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reduced shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(X_pca.shape)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201736852.png" alt="image-20220414201736852"></p>
<p>然后利用前两个主成分绘制cancer数据集的二维散点图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">mglearn.discrete_scatter(X_pca[:,<span class="number">0</span>], X_pca[:,<span class="number">1</span>],cancer.target)</span><br><span class="line">plt.legend(cancer.target_names,loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">plt.gca().set_aspect(<span class="string">&quot;equal&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;First principal component&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Second principal component&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201747284.png" alt="image-20220414201747284"></p>
<p>注意，pca是无监督算法，在寻找旋转方向时没有用到任何类别信息，但是可以看出两个类别被很好地区分开来了。</p>
<p>pca的缺点是解释性不好，主成分对应于原始数据中的方向，是原始特征的<strong>组合</strong>，这些组合往往非常复杂。</p>
<p>在拟合过程中，主成分被保存在PCA对象的components_属性中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PCA component shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(pca.components_.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PCA components:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(pca.components_))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201759498.png" alt="image-20220414201759498"></p>
<p>components_中的每一行对应于一个主成分，他们按照重要性排序（第一主成分排在第一位，以此类推），列对应于PCA的原始特征属性，共30个，例如mean radius，mean texture等。</p>
<p>用<strong>热图</strong>进行可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.matshow(pca.components_, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.yticks([<span class="number">0</span>,<span class="number">1</span>],[<span class="string">&quot;First component&quot;</span>,<span class="string">&quot;Second component&quot;</span>])</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(cancer.feature_names)), cancer.feature_names, rotation=<span class="number">60</span>, ha=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Principal components&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201812234.png" alt="image-20220414201812234"></p>
<p>这些所有特征的混合使得解释坐标轴含义变得十分困难。</p>
<blockquote>
<p>PCA用于人脸识别</p>
</blockquote>
<p>PCA的另一个应用是<strong>特征提取</strong>。</p>
<p>Wild数据集一共有3023张图像，每张大小为87*65像素，分别属于62个不同的人。但这个数据集有数据偏斜，例如乔治布什的有近530张图像，为降低数据偏斜，我们对每个人最多只取50张图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_lfw_people</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">people = fetch_lfw_people(min_faces_per_person=<span class="number">20</span>, resize=<span class="number">0.7</span>)</span><br><span class="line">image_shape = people.images[<span class="number">0</span>].shape</span><br><span class="line">fix, axes = plt.subplots(<span class="number">2</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>,<span class="number">8</span>), subplot_kw=&#123;<span class="string">&#x27;xticks&#x27;</span>: (), <span class="string">&#x27;yticks&#x27;</span>: ()&#125;)</span><br><span class="line"><span class="keyword">for</span> target, image, ax <span class="keyword">in</span> <span class="built_in">zip</span>(people.target, people.images, axes.ravel()):</span><br><span class="line">    ax.imshow(image)</span><br><span class="line">    ax.set_title(people.target_names[target])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201824757.png" alt="image-20220414201824757"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mask = np.zeros(people.target.shape, dtype=np.bool_)</span><br><span class="line"><span class="keyword">for</span> target <span class="keyword">in</span> np.unique(people.target):</span><br><span class="line">    mask[np.where(people.target == target)[<span class="number">0</span>][:<span class="number">50</span>]] = <span class="number">1</span></span><br><span class="line">X_people = people.data[mask]</span><br><span class="line">y_people = people.target[mask]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将灰度值缩放到0-1之间，而不是在0-255之间</span></span><br><span class="line"><span class="comment"># 以得到更好的数据稳定性</span></span><br><span class="line">X_people = X_people / <span class="number">255.</span></span><br></pre></td></tr></table></figure>
<p>利用单一近邻算法对人脸进行预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_people, y_people, stratify=y_people,random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 使用1个邻居构建分类器</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score of 1-nn: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(knn.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201836271.png" alt="image-20220414201836271"></p>
<p>利用PCA进行数据预处理：这里可以使用PCA的<strong>白化</strong>选项</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201848526.png" alt="image-20220414201848526"></p>
<p>上图是经过PCA白化（数据预处理）后的图像。</p>
<p>现在对训练数据拟合PCA对象，并提取前100个主成分，然后对训练数据和测试数据进行变换：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="comment"># 调用方法链，启用白化选项</span></span><br><span class="line">pca = PCA(n_components=<span class="number">100</span>, whiten=<span class="literal">True</span> ,random_state=<span class="number">0</span>).fit(X_train)</span><br><span class="line"><span class="comment"># 进行数据变换</span></span><br><span class="line">X_train_pca = pca.transform(X_train)</span><br><span class="line">X_test_pca = pca.transform(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train_pca.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train_pca.shape))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201858134.png" alt="image-20220414201858134"></p>
<p>可以看出新数据集有100个新特征，即前100个主成分。现在利用k近邻算法进行分类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">knn.fit(X_train_pca,y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score of 1-nn: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(knn.score(X_test_pca, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414201910291.png" alt="image-20220414201910291"></p>
<p>可见准确率得到了提升。</p>
<p>现在查看前几个主成分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fix, axes = plt.subplots(<span class="number">3</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>,<span class="number">12</span>), subplot_kw=&#123;<span class="string">&#x27;xticks&#x27;</span>: (), <span class="string">&#x27;yticks&#x27;</span>: ()&#125;)</span><br><span class="line"><span class="keyword">for</span> i, (component, ax) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(pca.components_, axes.ravel())):</span><br><span class="line">    ax.imshow(component.reshape(image_shape), cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">    ax.set_title(<span class="string">&quot;&#123;&#125;. component&quot;</span>.<span class="built_in">format</span>((i+<span class="number">1</span>)))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202415022.png" alt="image-20220414202415022"></p>
<p>理解主成分很难，但是可以猜测，例如左上图似乎主要编码的是人脸与背景的对比，第二主成分编码的可能是脸左半部分和右半部分的明暗程度差异。</p>
<h4 id="3-4-2-非负矩阵分解NMF"><a href="#3-4-2-非负矩阵分解NMF" class="headerlink" title="3.4.2 非负矩阵分解NMF"></a>3.4.2 非负矩阵分解NMF</h4><p>非负矩阵分解：non-negative matrix factorization。目的在于<code>提取有用特征</code>，这种方法只能应用于每个特征都是非负的数据，于PCA相比，NMF得到的分量更容易解释。</p>
<blockquote>
<p>NMF原理简介</p>
</blockquote>
<p>对于任意给定的一个非负矩阵V，其能够寻找到一个非负矩阵W和一个非负矩阵H，满足条件V=W<em>H,从而将一个非负的矩阵分解为左右两个非负矩阵的乘积。<em>*其中，V矩阵中每一列代表一个观测(observation)，每一行代表一个特征(feature)；W矩阵称为基矩阵，H矩阵称为系数矩阵或权重矩阵。这时用系数矩阵H代替原始矩阵，就可以实现对原始矩阵进行降维，得到数据特征的降维矩阵，从而减少存储空间。</em></em>过程如下图所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202452142.png" alt="image-20220414202452142"></p>
<p>其物理意义为每个基向量W(如眼睛、鼻子等等)乘以相应权重H最终组合而成V(脸)。虽然NMF是一个很厉害的算法，但其实质是加权和，我们可以在原理上等效为基本的线性方程。</p>
<blockquote>
<p>sklearn中封装的代码</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> NMF</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X, _ = load_iris(<span class="literal">True</span>) <span class="comment"># X为原始矩阵V</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># can be used for example for dimensionality reduction, source separation or topic extraction</span></span><br><span class="line"><span class="comment"># 个人认为最重要的参数是n_components、alpha、l1_ratio、solver</span></span><br><span class="line">nmf = NMF(n_components=<span class="number">2</span>,  <span class="comment"># k value,默认会保留全部特征</span></span><br><span class="line">          init=<span class="literal">None</span>,  <span class="comment"># W H 的初始化方法，包括&#x27;random&#x27; | &#x27;nndsvd&#x27;(默认) |  &#x27;nndsvda&#x27; | &#x27;nndsvdar&#x27; | &#x27;custom&#x27;.</span></span><br><span class="line">          solver=<span class="string">&#x27;cd&#x27;</span>,  <span class="comment"># &#x27;cd&#x27; | &#x27;mu&#x27;</span></span><br><span class="line">          beta_loss=<span class="string">&#x27;frobenius&#x27;</span>,  <span class="comment"># &#123;&#x27;frobenius&#x27;, &#x27;kullback-leibler&#x27;, &#x27;itakura-saito&#x27;&#125;，一般默认就好</span></span><br><span class="line">          tol=<span class="number">1e-4</span>,  <span class="comment"># 停止迭代的极限条件</span></span><br><span class="line">          max_iter=<span class="number">200</span>,  <span class="comment"># 最大迭代次数</span></span><br><span class="line">          random_state=<span class="literal">None</span>,</span><br><span class="line">          alpha=<span class="number">0.</span>,  <span class="comment"># 正则化参数</span></span><br><span class="line">          l1_ratio=<span class="number">0.</span>,  <span class="comment"># 正则化参数</span></span><br><span class="line">          verbose=<span class="number">0</span>,  <span class="comment"># 冗长模式</span></span><br><span class="line">          shuffle=<span class="literal">False</span>  <span class="comment"># 针对&quot;cd solver&quot;</span></span><br><span class="line">          )</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------函数------------------------</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;params:&#x27;</span>, nmf.get_params())  <span class="comment"># 获取构造函数参数的值，也可以nmf.attr得到，所以下面我会省略这些属性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面四个函数很简单，也最核心，例子中见</span></span><br><span class="line">nmf.fit(X)</span><br><span class="line">W = nmf.fit_transform(X)</span><br><span class="line">W = nmf.transform(X)</span><br><span class="line">nmf.inverse_transform(W)</span><br><span class="line"><span class="comment"># -----------------属性------------------------</span></span><br><span class="line">H = nmf.components_  <span class="comment"># H矩阵 系数矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;reconstruction_err_&#x27;</span>, nmf.reconstruction_err_)  <span class="comment"># 损失函数值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;n_iter_&#x27;</span>, nmf.n_iter_)  <span class="comment"># 实际迭代次数</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>将NMF应用于模拟数据</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202515413.png" alt="image-20220414202515413"></p>
<p>以上的图给出了NMF在二维模拟数据上的结果。对于两个分量的NMF，显然所有的数据点都可以写成这两个分量的正数组合，如果有足够的分量能够完美地重建数据（分量个数等于特征个数），那么算法会指向数据极值的方向。</p>
<p>NMF的主要参数是我们想要提取的分量个数。</p>
<blockquote>
<p>将NMF应用于信号源区分</p>
</blockquote>
<p>假设有一个信号由三个不同的信号源合成的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">S = mglearn.datasets.make_signals()</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">1</span>))</span><br><span class="line">plt.plot(S, <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Time&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Signal&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202530024.png" alt="image-20220414202530024"></p>
<p>我们想要将混合信号分解为原始分量，假设有100种方法来观测混合信号，每种方法都为我们提供了一系列的观测结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据混合成100维的状态</span></span><br><span class="line">A = np.random.RandomState(<span class="number">0</span>).uniform(size=(<span class="number">100</span>,<span class="number">3</span>))</span><br><span class="line">X = np.dot(S, A.T)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape of measurements: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X.shape))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202543634.png" alt="image-20220414202543634"></p>
<p>我们可以用NMF来还原这三个信号：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nmf = NMF(n_components=<span class="number">3</span>, random_state=<span class="number">42</span>)</span><br><span class="line">S_ = nmf.fit_transform(X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Recovered signal shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(S_.shape))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202555098.png" alt="image-20220414202555098"></p>
<p>添加了PCA方法用于比较：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据混合成100维的状态</span></span><br><span class="line">A = np.random.RandomState(<span class="number">0</span>).uniform(size=(<span class="number">100</span>,<span class="number">3</span>))</span><br><span class="line">X = np.dot(S, A.T)</span><br><span class="line">nmf = NMF(n_components=<span class="number">3</span>, random_state=<span class="number">42</span>)</span><br><span class="line">S_ = nmf.fit_transform(X)</span><br><span class="line">pca = PCA(n_components=<span class="number">3</span>)</span><br><span class="line">H = pca.fit_transform(X)</span><br><span class="line">models = [X, S, S_, H]</span><br><span class="line">names = [<span class="string">&#x27;Observations (first three measurements)&#x27;</span>, <span class="string">&#x27;True sources&#x27;</span>, <span class="string">&#x27;NMF recovered signals&#x27;</span>, <span class="string">&#x27;PCA recovered signals&#x27;</span>]</span><br><span class="line">fig, axes = plt.subplots(<span class="number">4</span>, figsize=(<span class="number">8</span>,<span class="number">4</span>), gridspec_kw=&#123;<span class="string">&#x27;hspace&#x27;</span>: <span class="number">.5</span>&#125;, subplot_kw=&#123;<span class="string">&#x27;xticks&#x27;</span>:(), <span class="string">&#x27;yticks&#x27;</span>:()&#125;)</span><br><span class="line"><span class="keyword">for</span> model, name, ax <span class="keyword">in</span> <span class="built_in">zip</span>(models, names, axes):</span><br><span class="line">    ax.set_title(name)</span><br><span class="line">    ax.plot(model[:, :<span class="number">3</span>],<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202606483.png" alt="image-20220414202606483"></p>
<p>可以看出NMF在发现原始信号源时得到了不错的结果，而PCA则失败了，仅使用第一个成分来解释数据中的大部分变化。</p>
<h4 id="3-4-3-用t-SNE进行流形学习"><a href="#3-4-3-用t-SNE进行流形学习" class="headerlink" title="3.4.3 用t-SNE进行流形学习"></a>3.4.3 用t-SNE进行流形学习</h4><blockquote>
<p>简介</p>
</blockquote>
<p>有一类用于可视化的算法叫做流形学习算法，其中特别有用的一个是t-SNE算法。</p>
<p>t-SNE全称为t-distributed Stochastic Neighbor Embedding，翻译为t-随机邻近嵌入，它是一种嵌入模型，能够将高维空间中的数据映射到低维空间中，并保留数据集的局部特性，该算法在论文中非常常见，主要用于<strong>高维数据的降维和可视化</strong>。</p>
<p>t-SNE可以算是目前效果最好的数据降维和可视化方法之一，当我们想对高维数据集进行分类，但又不清楚这个数据集有没有很好的可分性（同类之间间隔小、异类之间间隔大）时，可以通过t-SNE将数据投影到2维或3维空间中观察一下：如果在低维空间中具有可分性，则数据是可分的；如果在低维空间中不可分，则可能是因为数据集本身不可分，或者数据集中的数据不适合投影到低维空间。</p>
<p>t-SNE将数据点之间的相似度转化为条件概率，原始空间中数据点的相似度由高斯联合分布表示，嵌入空间中数据点的相似度由学生t分布表示。通过原始空间和嵌入空间的联合概率分布的KL散度（用于评估两个分布的相似度的指标，经常用于评估机器学习模型的好坏）来评估嵌入效果的好坏，即将有关KL散度的函数作为损失函数（loss function），通过梯度下降算法最小化损失函数，最终获得收敛结果。要注意t-SNE的缺点很明显：占用内存较多、运行时间长。</p>
<p><strong>此外</strong>：流形学习算法主要用于可视化，对t-SNE算法，不允许变换新数据，这意味着不能用于测试集，只能变换用于训练的数据。</p>
<blockquote>
<p>应用1——<strong>降维</strong></p>
</blockquote>
<p>输入4个5维的数据，通过t-SNE将其降维成2维的数据，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将3维数据降维2维</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4个3维的数据</span></span><br><span class="line">x = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">22</span>]])</span><br><span class="line"><span class="comment"># 嵌入空间的维度为2，即将数据降维成2维</span></span><br><span class="line">ts = TSNE(n_components=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">ts.fit_transform(x)</span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(ts.embedding_)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202817135.png" alt="image-20220414202817135"></p>
<blockquote>
<p>应用2——<strong>S型曲线的降维与可视化</strong></p>
</blockquote>
<p>S型曲线中的数据是高维的数据，不同的颜色表示不同的数据点。当我们通过t-SNE将数据嵌入到2维空间中后，<strong>可以看到数据点之间的类别信息被完整地保留了下来</strong>。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> manifold, datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成1000个S型曲线数据</span></span><br><span class="line">x, color = datasets._samples_generator.make_s_curve(n_samples=<span class="number">1000</span>, random_state=<span class="number">0</span>)		<span class="comment"># x是[1000,2]的2维数据，color是[1000,1]的一维数据</span></span><br><span class="line"></span><br><span class="line">n_neighbors = <span class="number">10</span></span><br><span class="line">n_components = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建自定义图像</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))		<span class="comment"># 指定图像的宽和高</span></span><br><span class="line">plt.suptitle(<span class="string">&quot;Dimensionality Reduction and Visualization of S-Curve Data &quot;</span>, fontsize=<span class="number">14</span>)		<span class="comment"># 自定义图像名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制S型曲线的3D图像</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">211</span>, projection=<span class="string">&#x27;3d&#x27;</span>)		<span class="comment"># 创建子图</span></span><br><span class="line">ax.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], x[:, <span class="number">2</span>], c=color, cmap=plt.cm.Spectral)		<span class="comment"># 绘制散点图，为不同标签的点赋予不同的颜色</span></span><br><span class="line">ax.set_title(<span class="string">&#x27;Original S-Curve&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax.view_init(<span class="number">4</span>, -<span class="number">72</span>)		<span class="comment"># 初始化视角</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># t-SNE的降维与可视化</span></span><br><span class="line">ts = manifold.TSNE(n_components=n_components, init=<span class="string">&#x27;pca&#x27;</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">y = ts.fit_transform(x)</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">plt.scatter(y[:, <span class="number">0</span>], y[:, <span class="number">1</span>], c=color, cmap=plt.cm.Spectral)</span><br><span class="line">ax1.set_title(<span class="string">&#x27;t-SNE Curve&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202828161.png" alt="image-20220414202828161"></p>
<blockquote>
<p>应用3——<strong>手写数字数据集的降维与可视化</strong></p>
</blockquote>
<p>手写数字数据集是一个经典的图片分类数据集，数据集中包含0-9这10个数字的灰度图片，每张图片以8*8共64个像素点表示。</p>
<p>首先利用PCA降维到二维可视化。对前两个主成分作图，并按类别对数据点着色。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> logging <span class="keyword">import</span> disable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line">digits = load_digits()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个PCA模型</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(digits.data)</span><br><span class="line"><span class="comment"># 将digits数据变换到前两个主成分的方向上</span></span><br><span class="line">digits_pca = pca.transform(digits.data)</span><br><span class="line">colors = [<span class="string">&quot;#476A2A&quot;</span>,<span class="string">&quot;#7851B8&quot;</span>,<span class="string">&quot;#BD3430&quot;</span>,<span class="string">&quot;#4A2D4E&quot;</span>,<span class="string">&quot;#875525&quot;</span>,<span class="string">&quot;#A83683&quot;</span>,<span class="string">&quot;#4E655E&quot;</span>,<span class="string">&quot;#853541&quot;</span>,<span class="string">&quot;#3A3120&quot;</span>,<span class="string">&quot;#535D8E&quot;</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.xlim(digits_pca[:,<span class="number">0</span>].<span class="built_in">min</span>(), digits_pca[:,<span class="number">0</span>].<span class="built_in">max</span>())</span><br><span class="line">plt.ylim(digits_pca[:,<span class="number">1</span>].<span class="built_in">min</span>(), digits_pca[:,<span class="number">1</span>].<span class="built_in">max</span>())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(digits.data)):</span><br><span class="line">    plt.text(digits_pca[i,<span class="number">0</span>], digits_pca[i,<span class="number">1</span>], <span class="built_in">str</span>(digits.target[i]), color=colors[digits.target[i]], fontdict=&#123;<span class="string">&#x27;weight&#x27;</span>: <span class="string">&#x27;bold&#x27;</span>, <span class="string">&#x27;size&#x27;</span>:<span class="number">9</span>&#125;)</span><br><span class="line">plt.xlabel(<span class="string">&quot;First principal component&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Second principal component&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202841091.png" alt="image-20220414202841091"></p>
<p>可以看出各个类别的界限不太好。现在利用t-sne算法进行降维并可视化，由于其算法不支持变换新数据，即没有transform方法，但可以调用fit_transform方法，其会构建模型并立即返回变换后的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line">tsne = TSNE(random_state=<span class="number">42</span>)</span><br><span class="line">digits_tsne = tsne.fit_transform(digits.data)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.xlim(digits_tsne[:,<span class="number">0</span>].<span class="built_in">min</span>(), digits_tsne[:,<span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span>)</span><br><span class="line">plt.ylim(digits_tsne[:,<span class="number">1</span>].<span class="built_in">min</span>(), digits_tsne[:,<span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(digits.data)):</span><br><span class="line">    plt.text(digits_tsne[i,<span class="number">0</span>], digits_tsne[i,<span class="number">1</span>], <span class="built_in">str</span>(digits.target[i]), color=colors[digits.target[i]], fontdict=&#123;<span class="string">&#x27;weight&#x27;</span>: <span class="string">&#x27;bold&#x27;</span>, <span class="string">&#x27;size&#x27;</span>:<span class="number">9</span>&#125;)</span><br><span class="line">plt.xlabel(<span class="string">&quot;t-SNE feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;t-SNE feature 1&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202858313.png" alt="image-20220414202858313"></p>
<p>所有类别都被明确分开，<strong>要记住</strong>，这种方法并不知道类别标签，它完全是无监督的，但它能够找到数据的一种二维表示，仅根据原始空间中数据点之间的靠近程度就能够将各个类别明确分开。</p>
<h3 id="3-5-聚类"><a href="#3-5-聚类" class="headerlink" title="3.5 聚类"></a>3.5 聚类</h3><p>聚类（clustering）是将数据集划分成组的任务，这些组称为<strong>簇</strong>。</p>
<blockquote>
<p>分类和聚类</p>
</blockquote>
<p><strong>分类：</strong>分类其实是从特定的数据中挖掘模式，作出判断的过程。比如Gmail邮箱里有垃圾邮件分类器，一开始的时候可能什么都不过滤，在日常使用过程中，我人工对于每一封邮件点选“垃圾”或“不是垃圾”，过一段时间，Gmail就体现出一定的智能，能够自动过滤掉一些垃圾邮件了。这是因为在点选的过程中，其实是给每一条邮件打了一个“标签”，这个标签只有两个值，要么是“垃圾”，要么“不是垃圾”，Gmail就会不断研究哪些特点的邮件是垃圾，哪些特点的不是垃圾，形成一些判别的模式，这样当一封信的邮件到来，就可以自动把邮件分到“垃圾”和“不是垃圾”这两个我们人工设定的分类的其中一个。</p>
<p><strong>聚类：</strong>聚类的目的也是把数据分类，但是事先我是不知道如何去分的，完全是算法自己来判断各条数据之间的相似性，相似的就放在一起。在聚类的结论出来之前，我完全不知道每一类有什么特点，一定要根据聚类的结果通过人的经验来分析，看看聚成的这一类大概有什么特点。</p>
<p>聚类和分类最大的不同在于：<strong>分类的目标是事先已知的，而聚类则不一样，聚类事先不知道目标变量是什么，类别没有像分类那样被预先定义出来。</strong></p>
<h4 id="3-5-1-k均值聚类"><a href="#3-5-1-k均值聚类" class="headerlink" title="3.5.1 k均值聚类"></a>3.5.1 k均值聚类</h4><p>聚类算法有很多种（几十种），K-Means是聚类算法中的最常用的一种，算法最大的特点是简单，好理解，运算速度快，但是<strong>只能应用于连续型</strong>的数据，并且一定要在聚类前需要手工指定要分成几类。</p>
<blockquote>
<p>算法步骤</p>
</blockquote>
<p>下面，我们描述一下K-means算法的过程，为了尽量不用数学符号，所以描述的不是很严谨，大概就是这个意思，“物以类聚、人以群分”：</p>
<ol>
<li>首先输入k的值，即我们希望将数据集经过聚类得到k个分组。</li>
<li>从数据集中随机选择k个数据点作为初始大哥（质心，Centroid）</li>
<li>对集合中每一个小弟，计算与每一个大哥的距离（距离的含义后面会讲），离哪个大哥距离近，就跟定哪个大哥。</li>
<li>这时每一个大哥手下都聚集了一票小弟，这时候召开人民代表大会，每一群选出新的大哥（其实是通过算法选出新的质心）。</li>
<li>如果新大哥和老大哥之间的距离小于某一个设置的阈值（表示重新计算的质心的位置变化不大，趋于稳定，或者说收敛），可以认为我们进行的聚类已经达到期望的结果，算法终止。</li>
<li>如果新大哥和老大哥距离变化很大，需要迭代3~5步骤。</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202912671.png" alt="image-20220414202912671"></p>
<p>上图中，三角形表示簇中心，圆形表示数据点。我们指定了要寻找三个簇。下图为算法的决策边界：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202922790.png" alt="image-20220414202922790"></p>
<blockquote>
<p>算法的简单实现</p>
</blockquote>
<p>下面给出它的Python实现，其中中心点的选取是手动选择的。在代码中随机产生了一个样本，用于测试K-means算法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># K-means Algorithm is a clustering algorithm</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_distance</span>(<span class="params">p1, p2</span>):</span></span><br><span class="line">    diff = [x-y <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(p1, p2)]</span><br><span class="line">    distance = np.sqrt(<span class="built_in">sum</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x**<span class="number">2</span>, diff)))</span><br><span class="line">    <span class="keyword">return</span> distance</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 计算多个点的中心</span></span><br><span class="line"><span class="comment"># cluster = [[1,2,3], [-2,1,2], [9, 0 ,4], [2,10,4]]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_center_point</span>(<span class="params">cluster</span>):</span></span><br><span class="line">    N = <span class="built_in">len</span>(cluster)</span><br><span class="line">    m = np.matrix(cluster).transpose().tolist()</span><br><span class="line">    center_point = [<span class="built_in">sum</span>(x)/N <span class="keyword">for</span> x <span class="keyword">in</span> m]</span><br><span class="line">    <span class="keyword">return</span> center_point</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 检查两个点是否有差别</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_center_diff</span>(<span class="params">center, new_center</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(center)</span><br><span class="line">    <span class="keyword">for</span> c, nc <span class="keyword">in</span> <span class="built_in">zip</span>(center, new_center):</span><br><span class="line">        <span class="keyword">if</span> c != nc:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># K-means算法的实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">K_means</span>(<span class="params">points, center_points</span>):</span></span><br><span class="line"> </span><br><span class="line">    N = <span class="built_in">len</span>(points)         <span class="comment"># 样本个数</span></span><br><span class="line">    n = <span class="built_in">len</span>(points[<span class="number">0</span>])      <span class="comment"># 单个样本的维度</span></span><br><span class="line">    k = <span class="built_in">len</span>(center_points)  <span class="comment"># k值大小</span></span><br><span class="line"> </span><br><span class="line">    tot = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:             <span class="comment"># 迭代</span></span><br><span class="line">        temp_center_points = [] <span class="comment"># 记录中心点</span></span><br><span class="line"> </span><br><span class="line">        clusters = []       <span class="comment"># 记录聚类的结果</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, k):</span><br><span class="line">            clusters.append([]) <span class="comment"># 初始化</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 针对每个点，寻找距离其最近的中心点（寻找组织）</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(points):</span><br><span class="line">            distances = []</span><br><span class="line">            <span class="keyword">for</span> center_point <span class="keyword">in</span> center_points:</span><br><span class="line">                distances.append(get_distance(data, center_point))</span><br><span class="line">            index = distances.index(<span class="built_in">min</span>(distances)) <span class="comment"># 找到最小的距离的那个中心点的索引，</span></span><br><span class="line"> </span><br><span class="line">            clusters[index].append(data)    <span class="comment"># 那么这个中心点代表的簇，里面增加一个样本</span></span><br><span class="line"> </span><br><span class="line">        tot += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(tot, <span class="string">&#x27;次迭代   &#x27;</span>, clusters)</span><br><span class="line">        k = <span class="built_in">len</span>(clusters)</span><br><span class="line">        colors = [<span class="string">&#x27;r.&#x27;</span>, <span class="string">&#x27;g.&#x27;</span>, <span class="string">&#x27;b.&#x27;</span>, <span class="string">&#x27;k.&#x27;</span>, <span class="string">&#x27;y.&#x27;</span>]  <span class="comment"># 颜色和点的样式</span></span><br><span class="line">        <span class="keyword">for</span> i, cluster <span class="keyword">in</span> <span class="built_in">enumerate</span>(clusters):</span><br><span class="line">            data = np.array(cluster)</span><br><span class="line">            data_x = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line">            data_y = [x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line">            plt.subplot(<span class="number">2</span>, <span class="number">3</span>, tot)</span><br><span class="line">            plt.plot(data_x, data_y, colors[i])</span><br><span class="line">            plt.axis([<span class="number">0</span>, <span class="number">1000</span>, <span class="number">0</span>, <span class="number">1000</span>])</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 重新计算中心点（该步骤可以与下面判断中心点是否发生变化这个步骤，调换顺序）</span></span><br><span class="line">        <span class="keyword">for</span> cluster <span class="keyword">in</span> clusters:</span><br><span class="line">            temp_center_points.append(calc_center_point(cluster))</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 在计算中心点的时候，需要将原来的中心点算进去</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, k):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(clusters[j]) == <span class="number">0</span>:</span><br><span class="line">                temp_center_points[j] = center_points[j]</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 判断中心点是否发生变化：即，判断聚类前后样本的类别是否发生变化</span></span><br><span class="line">        <span class="keyword">for</span> c, nc <span class="keyword">in</span> <span class="built_in">zip</span>(center_points, temp_center_points):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> check_center_diff(c, nc):</span><br><span class="line">                center_points = temp_center_points[:]   <span class="comment"># 复制一份</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment"># 如果没有变化，那么退出迭代，聚类结束</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> clusters <span class="comment"># 返回聚类的结果</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机获取一个样本集，用于测试K-means算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_data</span>():</span></span><br><span class="line"> </span><br><span class="line">    N = <span class="number">1000</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 产生点的区域</span></span><br><span class="line">    area_1 = [<span class="number">0</span>, N / <span class="number">4</span>, N / <span class="number">4</span>, N / <span class="number">2</span>]</span><br><span class="line">    area_2 = [N / <span class="number">2</span>, <span class="number">3</span> * N / <span class="number">4</span>, <span class="number">0</span>, N / <span class="number">4</span>]</span><br><span class="line">    area_3 = [N / <span class="number">4</span>, N / <span class="number">2</span>, N / <span class="number">2</span>, <span class="number">3</span> * N / <span class="number">4</span>]</span><br><span class="line">    area_4 = [<span class="number">3</span> * N / <span class="number">4</span>, N, <span class="number">3</span> * N / <span class="number">4</span>, N]</span><br><span class="line">    area_5 = [<span class="number">3</span> * N / <span class="number">4</span>, N, N / <span class="number">4</span>, N / <span class="number">2</span>]</span><br><span class="line"> </span><br><span class="line">    areas = [area_1, area_2, area_3, area_4, area_5]</span><br><span class="line">    k = <span class="built_in">len</span>(areas)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 在各个区域内，随机产生一些点</span></span><br><span class="line">    points = []</span><br><span class="line">    <span class="keyword">for</span> area <span class="keyword">in</span> areas:</span><br><span class="line">        rnd_num_of_points = random.randint(<span class="number">50</span>, <span class="number">200</span>)</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, rnd_num_of_points):</span><br><span class="line">            rnd_add = random.randint(<span class="number">0</span>, <span class="number">100</span>)</span><br><span class="line">            rnd_x = random.randint(area[<span class="number">0</span>] + rnd_add, area[<span class="number">1</span>] - rnd_add)</span><br><span class="line">            rnd_y = random.randint(area[<span class="number">2</span>], area[<span class="number">3</span>] - rnd_add)</span><br><span class="line">            points.append([rnd_x, rnd_y])</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 自定义中心点，目标聚类个数为5，因此选定5个中心点</span></span><br><span class="line">    center_points = [[<span class="number">0</span>, <span class="number">250</span>], [<span class="number">500</span>, <span class="number">500</span>], [<span class="number">500</span>, <span class="number">250</span>], [<span class="number">500</span>, <span class="number">250</span>], [<span class="number">500</span>, <span class="number">750</span>]]</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> points, center_points</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"> </span><br><span class="line">    points, center_points = get_test_data()</span><br><span class="line">    clusters = K_means(points, center_points)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;#######最终结果##########&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, cluster <span class="keyword">in</span> <span class="built_in">enumerate</span>(clusters):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;cluster &#x27;</span>, i, <span class="string">&#x27; &#x27;</span>, cluster)</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414202945532.png" alt="image-20220414202945532"></p>
<blockquote>
<p>sklearn中的算法使用</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">X, y = make_blobs()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建聚类模型</span></span><br><span class="line"><span class="comment"># 指定簇的个数为3，默认为8</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">kmeans.fit(X)</span><br></pre></td></tr></table></figure>
<p>算法会为X中的每个数据点分配一个簇标签</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Cluster memberships:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(kmeans.labels_))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203009442.png" alt="image-20220414203009442"></p>
<p>调用predict方法会为新数据点分配簇标签，返回同上的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(kmeans.predict(X))</span><br></pre></td></tr></table></figure>
<p>可以看出，每个数据点都有标签，但这些<strong>标签本身没有任何含义</strong>，算法<strong>唯一</strong>给出的信息就是同一个标签下的<strong>数据点非常相似</strong>而已。</p>
<p>当然我们也可以指定其他的簇的数量。</p>
<blockquote>
<p>k均值聚类算法失败的案例</p>
</blockquote>
<p>即使知道给定数据集中簇的正确个数，k均值可能也不是总能找到他们。k均值只能找到相对简单的形状，此外他还假设所有的簇在某种程度上具有相同的“直径”，所以他总是将簇之间的边界刚好画在簇的中心的中间位置。这会产生令人惊讶的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_varied, y_varied = make_blobs(n_samples=<span class="number">200</span>, cluster_std=[<span class="number">1.0</span>,<span class="number">2.5</span>,<span class="number">0.5</span>],random_state=<span class="number">170</span>)</span><br><span class="line">y_pred = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">0</span>).fit_predict(X_varied)</span><br><span class="line">mglearn.discrete_scatter(X_varied[:,<span class="number">0</span>], X_varied[:,<span class="number">1</span>], y_pred)</span><br><span class="line">plt.legend([<span class="string">&quot;cluster 0&quot;</span>, <span class="string">&quot;cluster 1&quot;</span>, <span class="string">&quot;cluster 2&quot;</span>], loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203028375.png" alt="image-20220414203028375"></p>
<p>可以看出，“应该在”中间零散的数据点组成的簇中的一些数据点，分布在了左下角和右上角密集簇中。</p>
<p>又如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">X, y = make_blobs(n_samples=<span class="number">600</span>, random_state=<span class="number">170</span>)</span><br><span class="line">rng = np.random.RandomState(<span class="number">74</span>)</span><br><span class="line"><span class="comment"># 变换数据使其拉长</span></span><br><span class="line">transformation = rng.normal(size=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">X = np.dot(X, transformation)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据聚类成3簇</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">kmeans.fit(X)</span><br><span class="line">y_pred = kmeans.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出簇分配和簇中心</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y_pred, cmap=mglearn.cm3)</span><br><span class="line">plt.scatter(kmeans.cluster_centers_[:,<span class="number">0</span>], kmeans.cluster_centers_[:,<span class="number">1</span>], marker=<span class="string">&#x27;^&#x27;</span>, c=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>], s=<span class="number">100</span>, linewidths=<span class="number">2</span>, cmap=mglearn.cm3)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203038877.png" alt="image-20220414203038877"></p>
<p>可以看出，数据中包含明确分开的三部分，但这三部分被沿着对角线方向拉长，由于k均值仅考虑到最近簇中心的距离，所以他无法处理这种类型的数据。</p>
<p>如果簇的形状更复杂，那么k均值的表现会更差。</p>
<blockquote>
<p>矢量量化，或者将k均值看作分解</p>
</blockquote>
<p>k均值尝试利用簇中心来表示每个数据点，可以将其看成仅用一个分量来表示每个数据点，该分量由簇中心给出，这种观点将k均值看作是一种分解方法，其中每个点用单一分量来表示，这种观点称为<strong>矢量量化</strong>。</p>
<p>对于k均值，重建就是在训练集中找到的最近的簇中心。</p>
<blockquote>
<p>总结</p>
</blockquote>
<p>k均值是非常流行的聚类算法，缺点在于它依赖于随机初始化，此外它对簇形状的假设的约束性较强，而且还要求指定所要寻找的簇的个数（现实应用中可能并不知道这个数字）。</p>
<h4 id="3-5-2-凝聚聚类"><a href="#3-5-2-凝聚聚类" class="headerlink" title="3.5.2 凝聚聚类"></a>3.5.2 凝聚聚类</h4><blockquote>
<p>简介</p>
</blockquote>
<p>层次聚类：是一种很直观的算法。顾名思义就是要一层一层地进行聚类，可以从下而上地把小的cluster合并聚集，也可以从上而下地将大的cluster进行分割。自下而上地进行聚类称为<strong>凝聚式层次聚类</strong>，自上而下地进行聚类称为<strong>分裂式层次聚类</strong>。似乎一般用得比较多的是从下而上地聚集。</p>
<ul>
<li>凝聚式：从点作为个体簇开始，每一步合并两个最接近的簇。</li>
<li>分裂式：从包含所有点的某个簇开始，每一步分裂一个簇，直到仅剩下单点簇。</li>
</ul>
<p>凝聚聚类（agglomerative clustering）指的是许多基于相同原则构建的聚类算法，这一原则是：算法首先声明每个点是自己的簇，然后合并两个最相似的簇，直到满足某种停止准则为止。scikit-learn中实现的停止准则是簇的个数，因此相似的簇被合并，直到仅剩下指定个数的簇。还有一些链接准则，规定如何度量“最相似的簇”。这种度量总是定义在两个现有的簇之间。</p>
<p>scikit-learn中提供三种选项：</p>
<ul>
<li>ward：默认选项，ward挑选两个簇来合并，使得所有簇中的<strong>方差增加最小</strong>。这通常会得到大小差不多相等的簇。</li>
<li>average：将簇中所有点之间平均距离最小的两个簇合并。</li>
<li>complete：也称为最大链接，将簇中点之间最大距离最小的两个簇合并。</li>
</ul>
<p>ward适用于大多数数据集，如果簇中的成员个数非常不同，那么可以使用后两者效果可能更好。</p>
<blockquote>
<p>算法步骤</p>
</blockquote>
<ol>
<li><p>如果需要，计算邻近度矩阵</p>
</li>
<li><p>repeat</p>
</li>
<li><p>合并最接近的两个簇</p>
</li>
<li><p>更新邻近度矩阵，以反映新的簇与原来的簇之间的邻近性</p>
</li>
<li><p>until 仅剩下一个簇/达到某个终止条件</p>
</li>
</ol>
<p>其中的概念有：</p>
<ul>
<li><strong>邻近度矩阵</strong></li>
</ul>
<p>邻近度有许多种定义方式，比如欧氏距离，曼哈顿距离，马氏距离，余弦相似度，Jaccard系数，Bregman散度等等。种类丰富，样品奇多，根据不同的需求来选择最适合的邻近度，计算得到相应的邻近度矩阵。</p>
<ul>
<li><strong>簇与簇之间邻近度的定义</strong></li>
</ul>
<p>每个簇中的点数不一定相等，如何计算两个不同簇之间的邻近度呢？</p>
<p>常用的有三种方法：<strong>单链（MIN准则</strong>），<strong>全链（MAX准则）</strong>，<strong>组平均技术</strong>。</p>
<ul>
<li>单链：单链（MIN）定义簇的邻近度为不同簇的两个最近的点之间的邻近度。在图中，即不同结点子集中两个节点之间的最短边。下图中的虚线，就是左右两个簇的邻近度。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203056206.png" alt="image-20220414203056206"></p>
<ul>
<li>全链（MAX）定义簇的邻近度为不同簇中两个最远的点之间的邻近度作为簇的邻近度。在图中，即不同结点子集中两个结点之间的最长边。下图中的虚线，就是左右两个簇的邻近度。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203214244.png" alt="image-20220414203214244"></p>
<ul>
<li>组平均是一种基于图的方法。它定义簇邻近度为取自不同簇的所有点对邻近度的平均值（平均长度）。</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203227635.png" alt="image-20220414203227635"></p>
<p>下图给出了在一个二维数据及上的凝聚聚类过程，指定要寻找三个簇：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203239904.png" alt="image-20220414203239904"></p>
<p>最开始，每个点自成一簇，然后在每一步中，相距最近的两个簇被合并。</p>
<p>凝聚算发不能对新数据点做出预测，因此没有predict方法，但是有fit_predict方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line">X, y = make_blobs(random_state=<span class="number">1</span>)</span><br><span class="line">agg = AgglomerativeClustering(n_clusters=<span class="number">3</span>)</span><br><span class="line">assignment = agg.fit_predict(X)</span><br><span class="line"></span><br><span class="line">mglearn.discrete_scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], assignment)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203254162.png" alt="image-20220414203254162"></p>
<p>可以看出，算法完美地完成了聚类。</p>
<blockquote>
<p>树状图可视化</p>
</blockquote>
<p>层次化的簇分配图如下：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203309912.png" alt="image-20220414203309912"></p>
<p>也可采用树状图显示这一过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.cluster.hierarchy <span class="keyword">import</span> dendrogram, ward</span><br><span class="line">X, y = make_blobs(random_state=<span class="number">0</span>, n_samples=<span class="number">12</span>)</span><br><span class="line">linkage_array = ward(X)</span><br><span class="line">dendrogram(linkage_array)</span><br><span class="line"></span><br><span class="line">ax = plt.gca()</span><br><span class="line">bounds = ax.get_xbound()</span><br><span class="line">ax.plot(bounds, [<span class="number">7.25</span>,<span class="number">7.25</span>], <span class="string">&#x27;--&#x27;</span>, c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">ax.plot(bounds, [<span class="number">4</span>,<span class="number">4</span>], <span class="string">&#x27;--&#x27;</span>, c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax.text(bounds[<span class="number">1</span>], <span class="number">7.25</span>, <span class="string">&#x27; two clusters&#x27;</span>, va=<span class="string">&#x27;center&#x27;</span>, fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">15</span>&#125;)</span><br><span class="line">ax.text(bounds[<span class="number">1</span>], <span class="number">4</span>, <span class="string">&#x27; three clusters&#x27;</span>, va=<span class="string">&#x27;center&#x27;</span>, fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Sample index&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Cluster distance&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203327125.png" alt="image-20220414203327125"></p>
<p>树状图在底部显示数据点，然后以这些点（表示单点簇）作为叶节点绘制一棵树，每合并两个簇就添加一个新的父节点。</p>
<blockquote>
<p>主要问题</p>
</blockquote>
<ol>
<li>缺乏全局目标函数</li>
</ol>
<p>凝聚层次聚类不能视为全局优化目标函数，因为在每一步合并时仅仅局部地确定哪些簇应当合并（或分裂，对于分裂式）。但是避开了解决困难的组合优化问题，并且这样的方法没有局部极小问题或难选择初始点的问题。</p>
<ol>
<li>处理不同大小簇的能力</li>
</ol>
<p>关于如何处理待合并的簇对的相对大小这个问题，解决的方法有两种：一是<strong>加权</strong>，就是不同簇中的点具有不同的权值；二是<strong>非加权</strong>，需要考虑每个簇的点数。</p>
<p>比如组平均技术，上面说的组平均实际上是组平均技术的非加权版，全称为“使用算术平均的非加权的对组方法”（UPGMA），Lance-Williams公式的系数也涉及每个被合并簇的大小。而对于加权版本（WPGMA），它的系数是常数：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://www.zhihu.com/equation?tex=\alpha+_{A}" alt="[公式]">=1/2，<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://www.zhihu.com/equation?tex=\alpha+_{B}" alt="[公式]">=1/2，β=0，γ=0。通常非加权方法更可取，除非出于某种原因个体点具有不同的权值，如对象类非均匀的抽样。</p>
<ol>
<li>合并决策是最终的</li>
</ol>
<p>对于合并两个簇，凝聚层次聚类可以使用所有点的逐对相似度信息趋向于作出好的局部决策。然而，一旦做出合并两个簇的决策，以后就不能撤销，这阻碍了局部最优标准变成全局最优标准。一些方法试图克服“合并是最终的”这一限制，如移动树的分支以改善全局目标；使用划分聚类方法（K均值）来创建许多小簇，然后从这些小簇出发进行层次聚类。</p>
<h4 id="3-5-3-DBSCAN"><a href="#3-5-3-DBSCAN" class="headerlink" title="3.5.3 DBSCAN"></a>3.5.3 DBSCAN</h4><blockquote>
<p>简介</p>
</blockquote>
<p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法）是一种基于密度的空间聚类算法。 该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据库中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。</p>
<p>主要优点在于不需要用户先验地设置簇的个数，可以划分具有复杂形状的簇，还可以找出不属于任何簇的点。</p>
<blockquote>
<p>算法过程</p>
</blockquote>
<p>DBSCAN算法有两个参数：eps和min_samples。</p>
<ul>
<li>eps：epsilon，为以某个点为圆心的圆的半径，默认为0.5</li>
<li>min_samples：在这个圆中至少含有数据点的个数</li>
</ul>
<p>DBSCAN算法将数据点类型分为三类：</p>
<ol>
<li><strong>核心点</strong>：在半径Eps内含有超过MinPts数目的点。</li>
<li><strong>边界点</strong>：在半径Eps内点的数量小于MinPts,但是落在核心点的邻域内的点。</li>
<li><strong>噪音点</strong>：既不是核心点也不是边界点的点。</li>
</ol>
<p>算法首先任意选取一个点，然后找到到这个点的距离小于等于eps的所有的点。如果所有点的个数小于min_samples，那么这个点被标记为噪声，不属于任何一个簇；否则这个点标记为核心点，并被分配一个新的簇标签。然后访问该点的所有邻居（在eps范围内），如果他们还没有簇标签，则将核心点的标签赋给他们。如果他们是核心点，那么就依次访问邻居，以此类推。直到在簇的eps距离内没有更多的核心点为止，然后选取另一个尚未被访问的点，并重复相同的过程。</p>
<p>以下展示了两个参数设置对聚类效果的影响：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203359665.png" alt="image-20220414203359665"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203408588.png" alt="image-20220414203408588"></p>
<p>上图中，较大点为核心点，较小的为边界点，没有颜色的为噪声。</p>
<p>虽然DBSCAN不需要显式地设置簇的个数，但是设置eps可以<strong>隐式</strong>地控制找到的簇的个数。</p>
<blockquote>
<p>在moon数据集上的表现</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">X, y = make_moons(n_samples=<span class="number">200</span>, noise=<span class="number">0.05</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="comment"># 将数据缩放成平均值为0，方差为1</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit(X)</span><br><span class="line">X_scaled = scaler.transform(X)</span><br><span class="line"></span><br><span class="line">dbscan = DBSCAN()</span><br><span class="line">clusters = dbscan.fit_predict(X_scaled)</span><br><span class="line"><span class="comment"># 绘制簇分配</span></span><br><span class="line">plt.scatter(X_scaled[:,<span class="number">0</span>], X_scaled[:,<span class="number">1</span>], c=clusters, cmap=mglearn.cm2, s=<span class="number">60</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203420906.png" alt="image-20220414203420906"></p>
<p>采用默认eps可以达到理想的效果，如果减小eps，可能会出现更多的簇，以下是去eps=0.2时的情况：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203430239.png" alt="image-20220414203430239"></p>
<h4 id="3-5-4-聚类算法的对比与评估"><a href="#3-5-4-聚类算法的对比与评估" class="headerlink" title="3.5.4 聚类算法的对比与评估"></a>3.5.4 聚类算法的对比与评估</h4><blockquote>
<p>用真实值评估聚类</p>
</blockquote>
<p>有一些指标可以用于评估聚类算法相对于真是聚类的结果。其中重要的有：</p>
<ul>
<li>调整兰德指数（adjusted rand index，ARI）： 取值在[-1,1]之间，负数代表结果不好，越接近于1越好；对任意数量的聚类中心和样本数，随机聚类的ARI都非常接近于0；可用于聚类算法之间的比较。缺点在于ARI需要真实的标签信息对聚类结果进行评价。</li>
<li>归一化互信息（normalized mutual information，NMI）：将互信息放在[0,1]之间，容易评价算法的好坏。</li>
</ul>
<p>下面是利用ARI来比较三种聚类算法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core.fromnumeric <span class="keyword">import</span> size</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> seed</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> cluster</span><br><span class="line"><span class="keyword">from</span> kmeans_proc <span class="keyword">import</span> K_means</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.cluster <span class="keyword">import</span> adjusted_rand_score</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans, AgglomerativeClustering, DBSCAN</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line">X, y = make_moons(n_samples=<span class="number">200</span>, noise=<span class="number">0.05</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据缩放成平均值为0，方差为1</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit(X)</span><br><span class="line">X_scaled = scaler.transform(X)</span><br><span class="line"></span><br><span class="line">fix, axes = plt.subplots(<span class="number">1</span>, <span class="number">4</span>, figsize=(<span class="number">15</span>,<span class="number">3</span>), subplot_kw=&#123;<span class="string">&#x27;xticks&#x27;</span>: (), <span class="string">&#x27;yticks&#x27;</span>: ()&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出要使用的算法</span></span><br><span class="line">algorithms = [KMeans(n_clusters=<span class="number">2</span>), AgglomerativeClustering(n_clusters=<span class="number">2</span>), DBSCAN()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个随机的簇分配，作为参考</span></span><br><span class="line">random_state = np.random.RandomState(seed=<span class="number">0</span>)</span><br><span class="line">random_clusters = random_state.randint(low=<span class="number">0</span>, high=<span class="number">2</span>, size=<span class="built_in">len</span>(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制随机分配</span></span><br><span class="line">axes[<span class="number">0</span>].scatter(X_scaled[:,<span class="number">0</span>], X_scaled[:,<span class="number">1</span>], c=random_clusters, cmap=mglearn.cm3, s=<span class="number">60</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&quot;Random assignment - ARI: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(adjusted_rand_score(y, random_clusters)))</span><br><span class="line"><span class="keyword">for</span> ax, algorithm <span class="keyword">in</span> <span class="built_in">zip</span>(axes[<span class="number">1</span>:], algorithms):</span><br><span class="line">    <span class="comment"># 绘制簇分配和簇中心</span></span><br><span class="line">    clusters = algorithm.fit_predict(X_scaled)</span><br><span class="line">    ax.scatter(X_scaled[:,<span class="number">0</span>], X_scaled[:,<span class="number">1</span>], c=clusters,cmap=mglearn.cm3, s=<span class="number">60</span>)</span><br><span class="line">    ax.set_title(<span class="string">&quot;&#123;&#125; - ARI: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(algorithm.__class__.__name__,adjusted_rand_score(y, clusters)))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203457866.png" alt="image-20220414203457866"></p>
<p>从上图可以看出，调整兰德指数给出了符合直觉的结果。</p>
<p>注意：用这种真实值方式评估聚类时，一个常见的错误就是使用accuracy_score而不是ARI或NMI或其他聚类指标来评估聚类算法，使用精度的<strong>问题</strong>在于，他要求分配的簇标签与真实值完全匹配，但簇标签本身毫无意义——唯一重要的是哪些点位于同一个簇中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="comment"># 这两种点标签对应于相同的聚类</span></span><br><span class="line">clusters1 = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">clusters2 = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 精度为0，因为两者标签完全不同</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(accuracy_score(clusters1, clusters2)))</span><br><span class="line"><span class="comment"># 调整兰德指数为1，因为两者的聚类完全相同</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;ARI: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(adjusted_rand_score(clusters1, clusters2)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203507543.png" alt="image-20220414203507543"></p>
<blockquote>
<p>在没有真实值的情况下评估聚类</p>
</blockquote>
<ul>
<li>轮廓系数：轮廓分数计算一个簇的紧致度，其值越大越好，最高为1，但是对于复杂形状效果不好。</li>
</ul>
<p><code>算法</code>：假设我们已经通过一定算法，将待分类数据进行了聚类。常用的比如使用K-means ，将待分类数据分为了 k 个簇 。对于簇中的每个向量。分别计算它们的轮廓系数。</p>
<p>对于其中的一个点 i 来说：</p>
<p>计算 a(i) = average(i向量到所有它属于的簇中其它点的距离)</p>
<p>计算 b(i) = min (i向量到与它相邻最近的一簇内的所有点的平均距离)</p>
<p>那么 i 向量轮廓系数就为：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203519813.png" alt="image-20220414203519813"></p>
<p>可见轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优。<strong>将所有点的轮廓系数求平均，就是该聚类结果总的轮廓系数</strong>。a(i) ：i向量到同一簇内其他点<strong>不相似程度</strong>的平均值；b(i) ：i向量到其他簇的平均<strong>不相似程度</strong>的最小值。</p>
<hr>
<p>以下是利用轮廓系数对三种算法的评估：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.cluster <span class="keyword">import</span> silhouette_score</span><br><span class="line">X, y = make_moons(n_samples=<span class="number">200</span>, noise=<span class="number">0.05</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据缩放成平均值为0，方差为1</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit(X)</span><br><span class="line">X_scaled = scaler.transform(X)</span><br><span class="line"></span><br><span class="line">fix, axes = plt.subplots(<span class="number">1</span>, <span class="number">4</span>, figsize=(<span class="number">15</span>,<span class="number">3</span>), subplot_kw=&#123;<span class="string">&#x27;xticks&#x27;</span>: (), <span class="string">&#x27;yticks&#x27;</span>: ()&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出要使用的算法</span></span><br><span class="line">algorithms = [KMeans(n_clusters=<span class="number">2</span>), AgglomerativeClustering(n_clusters=<span class="number">2</span>), DBSCAN()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个随机的簇分配，作为参考</span></span><br><span class="line">random_state = np.random.RandomState(seed=<span class="number">0</span>)</span><br><span class="line">random_clusters = random_state.randint(low=<span class="number">0</span>, high=<span class="number">2</span>, size=<span class="built_in">len</span>(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制随机分配</span></span><br><span class="line">axes[<span class="number">0</span>].scatter(X_scaled[:,<span class="number">0</span>], X_scaled[:,<span class="number">1</span>], c=random_clusters, cmap=mglearn.cm3, s=<span class="number">60</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&quot;Random assignment: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(silhouette_score(X_scaled, random_clusters)))</span><br><span class="line"><span class="keyword">for</span> ax, algorithm <span class="keyword">in</span> <span class="built_in">zip</span>(axes[<span class="number">1</span>:], algorithms):</span><br><span class="line">    <span class="comment"># 绘制簇分配和簇中心</span></span><br><span class="line">    clusters = algorithm.fit_predict(X_scaled)</span><br><span class="line">    ax.scatter(X_scaled[:,<span class="number">0</span>], X_scaled[:,<span class="number">1</span>], c=clusters,cmap=mglearn.cm3, s=<span class="number">60</span>)</span><br><span class="line">    ax.set_title(<span class="string">&quot;&#123;&#125; : &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(algorithm.__class__.__name__,silhouette_score(X_scaled, clusters)))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203547178.png" alt="image-20220414203547178"></p>
<p>可以看出，k均值分数最高，但实际聚类效果不如分数较低的DBSCAN算法。</p>
<h4 id="3-5-5-聚类方法小结"><a href="#3-5-5-聚类方法小结" class="headerlink" title="3.5.5 聚类方法小结"></a>3.5.5 聚类方法小结</h4><p>聚类的应用于评估是一个非常定性的过程，通常在数据分析的探索阶段很有帮助。</p>
<p>k均值可以用簇的平均值来表示簇，他还可以看作一种分解方法，每个数据点都有其簇中心表示。</p>
<p>DBSCAN可以检测到没有分配任何簇的噪声点，还可以帮助自动判断簇的数量，它允许簇有复杂的形状。</p>
<p>凝聚聚类可以提供数据的可能划分的整个层次结构，可以通过树状图轻松查看。</p>
<h2 id="第四章-数据表示与特征工程"><a href="#第四章-数据表示与特征工程" class="headerlink" title="第四章 数据表示与特征工程"></a>第四章 数据表示与特征工程</h2><ul>
<li><p>连续特征：数据由浮点数组成。 </p>
</li>
<li><p>离散特征（或分类特征）：这种特征通常不是数值，不以连续的方式变化，例如产品的品牌，颜色，部门等。</p>
</li>
</ul>
<p>数据表示方式都会对机器学习模型的性能产生巨大影响。对于某个特定应用，如何找到最佳数据表示，这个问题被称为<strong>特征工程</strong>。</p>
<h3 id="4-1-分类变量"><a href="#4-1-分类变量" class="headerlink" title="4.1 分类变量"></a>4.1 分类变量</h3><p>adult数据集：任务是预测一名工人的收入是高于5w，还是低于5w，属于分类任务，在这个数据集中age，hours-per-week属于连续特征，但是workclass，education，gender等属于离散特征，他们都来自于一系列固定的可能取值（而不是一个范围），表示的是定性属性（而不是数量）。</p>
<p>我们需要换一种方式来表示数据。</p>
<h4 id="4-1-1-One-Hot编码（虚拟变量）"><a href="#4-1-1-One-Hot编码（虚拟变量）" class="headerlink" title="4.1.1 One-Hot编码（虚拟变量）"></a>4.1.1 One-Hot编码（虚拟变量）</h4><p>表示分类变量最常用的方法就是使用one-hot编码（或N取1编码），也叫虚拟变量。虚拟变量背后的思想是将<strong>一个分类变量替换为一个或多个新特征</strong>，新特征取值为0或1。</p>
<p>将数据转换为分类变量的one-hot编码常常使用pandas。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">&quot;data/adult.data&quot;</span>, header=<span class="literal">None</span>, index_col=<span class="literal">False</span>, names=[</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;workclass&#x27;</span>,<span class="string">&#x27;fnlwgt&#x27;</span>,<span class="string">&#x27;education&#x27;</span>,<span class="string">&#x27;education-num&#x27;</span>,<span class="string">&#x27;martial-status&#x27;</span>,<span class="string">&#x27;occupation&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;relationship&#x27;</span>,<span class="string">&#x27;race&#x27;</span>,<span class="string">&#x27;gender&#x27;</span>,<span class="string">&#x27;capital-gain&#x27;</span>,<span class="string">&#x27;capital-loss&#x27;</span>,<span class="string">&#x27;hours-per-week&#x27;</span>,<span class="string">&#x27;native-country&#x27;</span>,<span class="string">&#x27;income&#x27;</span></span><br><span class="line">])</span><br><span class="line"><span class="comment"># </span></span><br><span class="line">data = data[[<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;workclass&#x27;</span>,<span class="string">&#x27;education&#x27;</span>,<span class="string">&#x27;gender&#x27;</span>,<span class="string">&#x27;hours-per-week&#x27;</span>,<span class="string">&#x27;occupation&#x27;</span>,<span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">display(data.head())</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203614356.png" alt="image-20220414203614356"></p>
<p>用pandas编码数据可以使用get_dummies函数，它将自动变换所有具有对象类型（例如字符串）的列或所有分类的列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Original features:\n&quot;</span>, <span class="built_in">list</span>(data.columns), <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">data_dummies = pd.get_dummies(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Features after get_dummies:\n&quot;</span>, <span class="built_in">list</span>(data_dummies.columns))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203630326.png" alt="image-20220414203630326"></p>
<p>可以看出，连续特征age和hours-per-week没有发生变化，而分类特征的每个可能取值都被扩展为一个新特征：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203640001.png" alt="image-20220414203640001"></p>
<blockquote>
<p>训练模型</p>
</blockquote>
<p>下面使用values属性将data_dummies数据框转换为NumPy数组，然后在其上训练一个机器学习模型。在训练模型之前，要把两个目标变量（现在被编码为两个income列）从数据中分离出来。</p>
<p>常见<strong>错误</strong>：将输出变量或输出变量的一些导出属性包含在特征表示中，是构建监督学习模型的一个常见错误。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">features = data_dummies.loc[:,<span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;occupation_ Transport-moving&#x27;</span>]</span><br><span class="line"><span class="comment"># 提取NumPy数组</span></span><br><span class="line">X = features.values</span><br><span class="line">y = data_dummies[<span class="string">&#x27;income_ &gt;50K&#x27;</span>].values</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X.shape: &#123;&#125; y.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X.shape, y.shape))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203648795.png" alt="image-20220414203648795"></p>
<p>现在的数据表示就可以被scikit-learn处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train ,X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">logreg = LogisticRegression()</span><br><span class="line">logreg.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(logreg.score(X_test, y_test))) </span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203656231.png" alt="image-20220414203656231"></p>
<h4 id="4-1-2-数字可以编码分类变量"><a href="#4-1-2-数字可以编码分类变量" class="headerlink" title="4.1.2 数字可以编码分类变量"></a>4.1.2 数字可以编码分类变量</h4><p>分类特征通常用整数进行编码，它们是数字并不意味着它们必须被视为连续特征。一个整数特征被视为连续的还是离散的，有时并不明确。</p>
<p>pandas的get_dummies函数将所有数字看作是连续特征，不会为其创建虚拟变量。可以将数据框中的数值列转换成字符串，再进行处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">demo_df = pd.DataFrame(&#123;<span class="string">&#x27;Integer Feature&#x27;</span>: [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>], <span class="string">&#x27;Categorical Feature&#x27;</span>: [<span class="string">&#x27;socks&#x27;</span>,<span class="string">&#x27;fox&#x27;</span>,<span class="string">&#x27;socks&#x27;</span>,<span class="string">&#x27;bix&#x27;</span>]&#125;)</span><br><span class="line">display(demo_df)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203703932.png" alt="image-20220414203703932"></p>
<p>以上是包含分类字符串和整数特征的数据框。</p>
<p>使用get_dummies函数只会编码字符串特征，不会改变整数特征：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display(pd.get_dummies(demo_df))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203712867.png" alt="image-20220414203712867"></p>
<p>如果想为Interger Feature这一列创建虚拟变量，可以使用columns参数显式地给出想要编码的列，于是两个特征都会被当做分类处理特征处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">demo_df[<span class="string">&#x27;Integer Feature&#x27;</span>] = demo_df[<span class="string">&#x27;Integer Feature&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">demo_hot = pd.get_dummies(demo_df, columns=[<span class="string">&#x27;Integer Feature&#x27;</span>, <span class="string">&#x27;Categorical Feature&#x27;</span>])</span><br><span class="line">display(demo_hot)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203721756.png" alt="image-20220414203721756"></p>
<h3 id="4-2-分箱，离散化，线性模型与树"><a href="#4-2-分箱，离散化，线性模型与树" class="headerlink" title="4.2 分箱，离散化，线性模型与树"></a>4.2 分箱，离散化，线性模型与树</h3><p>数据表示的最佳方法不仅取决于数据的语义，还取决于所使用的的模型种类。以下是在wave数据集上比较线性回归和决策树的图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> mglearn.datasets <span class="keyword">import</span> make_wave</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X ,y = make_wave(n_samples=<span class="number">100</span>)</span><br><span class="line">line = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">1000</span>, endpoint=<span class="literal">False</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">reg = DecisionTreeRegressor(min_samples_split=<span class="number">3</span>).fit(X, y)</span><br><span class="line">plt.plot(line, reg.predict(line), label=<span class="string">&quot;decision tree&quot;</span>)</span><br><span class="line"></span><br><span class="line">reg = LinearRegression().fit(X, y)</span><br><span class="line">plt.plot(line, reg.predict(line), label=<span class="string">&quot;linear regression&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(X[:,<span class="number">0</span>], y, <span class="string">&#x27;o&#x27;</span>, c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Regression output&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Input feature&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203732221.png" alt="image-20220414203732221"></p>
<p>有一种方法可以让<strong>线性模型</strong>在<strong>连续数据</strong>上变得更加强大，就是利用特征<strong>分箱</strong>（或者离散化）将其划分为多个特征。</p>
<p>将特征的输入范围划分成固定个数的箱子（bin），比如10个，那么数据点就可以利用它们所在的箱子来表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在-3,3的区间上划分十个段（箱子）</span></span><br><span class="line">bins = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">11</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bins: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(bins))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203744087.png" alt="image-20220414203744087"></p>
<p>接下来记录每个数据点所属的箱子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">which_bin = np.digitize(X, bins=bins)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nData points:\n&quot;</span>, X[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nBin membership for data points:\n&quot;</span>, which_bin[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203752427.png" alt="image-20220414203752427"></p>
<p>这里做的就是将wave数据集中单个连续输入特征变换为一个分类特征（离散特征）。</p>
<p>下面将这个离散特征用one-hot编码表示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="comment"># 使用OneHotEncoder进行变换</span></span><br><span class="line">encoder = OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># encoder.fit找到which_bin中的唯一值</span></span><br><span class="line">encoder.fit(which_bin)</span><br><span class="line"><span class="comment"># transform创建one-hot编码</span></span><br><span class="line">X_binned = encoder.transform(which_bin)</span><br><span class="line"><span class="built_in">print</span>(X_binned[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203800634.png" alt="image-20220414203800634"></p>
<p>由于我们指定了10个箱子，因此变换后的X_binned数据集现在包含10个特征。</p>
<p>现在在X_binned数据集上进行构建模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">line_binned = encoder.transform(np.digitize(line, bins=bins))</span><br><span class="line"></span><br><span class="line">reg = DecisionTreeRegressor(min_samples_split=<span class="number">3</span>).fit(X_binned, y)</span><br><span class="line">plt.plot(line, reg.predict(line_binned), label=<span class="string">&quot;decision tree&quot;</span>)</span><br><span class="line">reg = LinearRegression().fit(X_binned, y)</span><br><span class="line">plt.plot(line, reg.predict(line_binned), label=<span class="string">&quot;linear regression&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(X[:,<span class="number">0</span>], y, <span class="string">&#x27;o&#x27;</span>, c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Regression output&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Input feature&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203809875.png" alt="image-20220414203809875"></p>
<p>线性模型和决策树的线段完全重合，说明两者做出了相同的预测。对于每个箱子，二者都预测了同一个常数值。实际上每个线段的拐点就是两个箱子的边界。可以看出分箱，让线性模型的表现力得到了一定的提高。</p>
<blockquote>
<p>适用范围</p>
</blockquote>
<p>对于特定的数据集，如果有理由选择使用线性模型，例如<strong>数据集很大，维度很高，但有些特征与输出的关系是非线性的</strong>，那么分箱是提高建模能力的好方法。</p>
<h3 id="4-3-交互特征与多项式特征"><a href="#4-3-交互特征与多项式特征" class="headerlink" title="4.3 交互特征与多项式特征"></a>4.3 交互特征与多项式特征</h3><p>想要丰富特征表示，特别是对于线性模型而言，另一种方式是添加原始特征的<strong>交互特征和多项式特征</strong>。</p>
<blockquote>
<p>交互特征</p>
</blockquote>
<p>在上面的例子中，加入原始特征（即x轴），这样会得到11维的数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_binned = encoder.transform(which_bin)</span><br><span class="line">X_combined = np.hstack([X, X_binned])</span><br><span class="line"><span class="built_in">print</span>(X_combined.shape)</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203819830.png" alt="image-20220414203819830"></p>
<p>现在利用X_combined数据集构建模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">reg = LinearRegression().fit(X_combined, y)</span><br><span class="line">line_combined = np.hstack([line, line_binned])</span><br><span class="line">plt.plot(line, reg.predict(line_combined), label=<span class="string">&quot;linear regression combined&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">bin</span> <span class="keyword">in</span> bins:</span><br><span class="line">    plt.plot([<span class="built_in">bin</span>,<span class="built_in">bin</span>], [-<span class="number">3</span>,<span class="number">3</span>], <span class="string">&#x27;:&#x27;</span>, c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Regression output&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Input feature&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">plt.plot(X[:,<span class="number">0</span>], y, <span class="string">&#x27;o&#x27;</span>, c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203828991.png" alt="image-20220414203828991"></p>
<p>可以看出，模型学习到了一个向下的斜率，每个箱子都是相同的，现在我们要让每个箱子学习到不同的斜率，我们可以添加一个<strong>交互特征</strong>，这个特征是箱子指示符与原始特征的乘积。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将X_binned数据集变换，添加交互特征</span></span><br><span class="line">X_product = np.hstack([X_binned, X * X_binned])</span><br><span class="line">reg = LinearRegression().fit(X_product, y)</span><br><span class="line"></span><br><span class="line">line_product = np.hstack([line_binned, line * line_binned])</span><br><span class="line"></span><br><span class="line">plt.plot(line, reg.predict(line_product), label=<span class="string">&quot;linear regression product&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">bin</span> <span class="keyword">in</span> bins:</span><br><span class="line">    plt.plot([<span class="built_in">bin</span>,<span class="built_in">bin</span>], [-<span class="number">3</span>,<span class="number">3</span>], <span class="string">&#x27;:&#x27;</span>, c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Regression output&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Input feature&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">plt.plot(X[:,<span class="number">0</span>], y, <span class="string">&#x27;o&#x27;</span>, c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203838724.png" alt="image-20220414203838724"></p>
<p>现在模型中每个箱子都有自己的偏移和斜率。</p>
<blockquote>
<p>多项式特征</p>
</blockquote>
<p>另一种方法是使用原始特征的多项式特征。对于给定特征x，可以考虑x乘2，乘3，平方等等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="comment"># 包含直到x**10的多项式</span></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">10</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">poly.fit(X)</span><br><span class="line">X_poly = poly.transform(X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_poly.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_poly.shape))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203846840.png" alt="image-20220414203846840"></p>
<p>此时，经过变换后的X_poly数据集的特征为x，一直乘到10，共有10个特征。</p>
<p>将多项式特征与线性回归模型一起使用，可以得到经典的<strong>多项式回归模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">reg = LinearRegression().fit(X_poly, y)</span><br><span class="line">line_poly = poly.transform(line)</span><br><span class="line"></span><br><span class="line">plt.plot(line, reg.predict(line_poly), label=<span class="string">&quot;polynomial linear regression&quot;</span>)</span><br><span class="line">plt.plot(X[:,<span class="number">0</span>], y, <span class="string">&#x27;o&#x27;</span>, c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Regression output&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Input feature&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203859194.png" alt="image-20220414203859194"></p>
<p>多项式特征在这个一维数据上得到了非常平滑的拟合，但高次多项式在边界上或数据很少的区域上可能会有极端的表现。</p>
<p>作为对比，下面是在原始数据上学到的核SVM模型，没有做任何变换：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"><span class="keyword">for</span> gamma <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">10</span>]:</span><br><span class="line">    svr = SVR(gamma=gamma).fit(X, y)</span><br><span class="line">    plt.plot(line, svr.predict(line), label=<span class="string">&#x27;SVR gamma=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gamma))</span><br><span class="line">plt.plot(X[:,<span class="number">0</span>], y, <span class="string">&#x27;o&#x27;</span>, c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Regression output&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Input feature&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">plt.show()  </span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203908378.png" alt="image-20220414203908378"></p>
<blockquote>
<p>多项式特征的构造方式和作用</p>
</blockquote>
<p>先使用boston数据集进行说明：首先加载数据，然后利用MinMaxScaler将其缩放到0到1之间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.core.fromnumeric <span class="keyword">import</span> transpose</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">boston = load_boston()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 缩放数据</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">X_train_scaled = scaler.fit_transform(X_train)</span><br><span class="line">X_test_scaled = scaler.fit_transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面提取交互特征和多项式特征，次数最高为2</span></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">2</span>).fit(X_train_scaled)</span><br><span class="line">X_trian_poly = poly.transform(X_train_scaled)</span><br><span class="line">X_test_poly = poly.transform(X_test_scaled)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train_poly.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_trian_poly.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Polynomial feature names:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(poly.get_feature_names()))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203917243.png" alt="image-20220414203917243"></p>
<p>原始数据有13个特征，现在被扩展到105个特征，具体如上面所示。第一个特征为常数特征，为‘1’，接下来13个特征为原始特征，然后是为第一个特征的平方，以及它与其他特征的组合。</p>
<p>下面利用岭回归Ridge在有交互特征的数据上和没有交互特征的数据上进行性能对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"></span><br><span class="line">ridge = Ridge().fit(X_train_scaled, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Score without interactions: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(ridge.score(X_test_scaled, y_test)))</span><br><span class="line">ridge = Ridge().fit(X_trian_poly, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Score without interactions: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(ridge.score(X_test_poly, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203925771.png" alt="image-20220414203925771"></p>
<p>显然，在使用岭回归时，交互特征和多项式特征对性能提升有很大的帮助，但如果使用更加复杂的模型（比如随机森林），情况会稍有不同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">100</span>).fit(X_train_scaled, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Score without interactions: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(rf.score(X_test_scaled, y_test)))</span><br><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">100</span>).fit(X_trian_poly, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Score without interactions: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(rf.score(X_test_poly, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203933263.png" alt="image-20220414203933263"></p>
<p>没有额外的特征，随机森林的性能也要优于岭回归，但是可以看出，添加了交互特征和多项式特征后，实际上会略微降低随机森林的性能。</p>
<h3 id="4-4-单变量非线性变换"><a href="#4-4-单变量非线性变换" class="headerlink" title="4.4 单变量非线性变换"></a>4.4 单变量非线性变换</h3><p>其它变换通常对某些特征也有作用，特别是应用数学函数，例如log，exp或sin。大部分模型都在每个特征大致遵循高斯分布时表现最好，下面的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">rnd = np.random.RandomState(<span class="number">0</span>)</span><br><span class="line">X_org = rnd.normal(size=(<span class="number">1000</span>,<span class="number">3</span>))</span><br><span class="line">w = rnd.normal(size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">X = rnd.poisson(<span class="number">10</span> * np.exp(X_org))</span><br><span class="line">y = np.dot(X_org, w)</span><br><span class="line"></span><br><span class="line">bins = np.bincount(X[:,<span class="number">0</span>])</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(bins)), bins, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Number of appearnces&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Value&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203941259.png" alt="image-20220414203941259"></p>
<p>利用这个数据集拟合一个岭回归模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">score = Ridge().fit(X_train, y_train).score(X_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(score))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203949356.png" alt="image-20220414203949356"></p>
<p>现在对特征应用log变换：变换之后，数据分布的不对称性变小，也没有非常大的异常值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_train_log = np.log(X_train + <span class="number">1</span>)</span><br><span class="line">X_test_log = np.log(X_test + <span class="number">1</span>)</span><br><span class="line">plt.hist(X_train_log[:,<span class="number">0</span>], bins=<span class="number">25</span>, color=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Number of appearnces&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Value&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414203957711.png" alt="image-20220414203957711"></p>
<p>然后拟合一个岭回归模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score = Ridge().fit(X_train_log, y_train).score(X_test_log, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(score))</span><br></pre></td></tr></table></figure>
<p>在所举的例子中，所有特征都具有相同的性质。通常来说，只有一部分特征应该进行变换，有时每个特征的变换方式也各不相同。</p>
<h3 id="4-5-自动化特征选择"><a href="#4-5-自动化特征选择" class="headerlink" title="4.5 自动化特征选择"></a>4.5 自动化特征选择</h3><p>在添加新特征或处理一般的高维数据集时，最好将特征的数量减少到只包含最有用的那些特征，并且删除其余特征，这样会得到泛化能力更好，更简单的模型。如何判断每个特征的作用，方法主要有三种：</p>
<ul>
<li>单变量统计（univariate statistics）</li>
<li>基于模型的选择（model-based selection）</li>
<li>迭代选择（iterative selection）</li>
</ul>
<h4 id="4-5-1-单变量统计"><a href="#4-5-1-单变量统计" class="headerlink" title="4.5.1 单变量统计"></a>4.5.1 单变量统计</h4><p>在单变量统计中，我们计算每个特征和目标值之间的关系是否存在统计显著性，然后选择具有最高置信度的特征。这个方法只单独考虑每个特征，如果一个特征只有在与另一个特征合并时才具有信息量，那么这个特征将被舍弃。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">rom sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectPercentile</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得确定性的随机数</span></span><br><span class="line">rng = np.random.RandomState(<span class="number">42</span>)</span><br><span class="line">noise = rng.normal(size=(<span class="built_in">len</span>(cancer.data), <span class="number">50</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加噪声特征</span></span><br><span class="line"><span class="comment"># 前30个来自数据集，后50个是噪声</span></span><br><span class="line">X_w_noise = np.hstack([cancer.data, noise])</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_w_noise, cancer.target, random_state=<span class="number">0</span>, test_size=<span class="number">.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用f_classif(默认值)和SelectPercentile来选择50%的特征</span></span><br><span class="line">select = SelectPercentile(percentile=<span class="number">50</span>)</span><br><span class="line">select.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对训练集进行变换</span></span><br><span class="line">X_train_selected = select.transform(X_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train_selected.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train_selected.shape))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414204016423.png" alt="image-20220414204016423"></p>
<p>特征的数量从80减少到40个。布尔遮罩如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mask = select.get_support()</span><br><span class="line"><span class="built_in">print</span>(mask)</span><br><span class="line"><span class="comment"># 将遮罩可视化，黑色为被选中，白色为舍弃</span></span><br><span class="line">plt.matshow(mask.reshape(<span class="number">1</span>,-<span class="number">1</span>), cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Sample index&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414204027348.png" alt="image-20220414204027348"></p>
<p>可以看出大多数所选择的特征都是原始特征，并且大多数噪声特征都已被删除。下面是Logistic回归的拟合情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集进行变换</span></span><br><span class="line">X_test_selected = select.transform(X_test)</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Score with all features: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_test, y_test)))</span><br><span class="line">lr.fit(X_train_selected, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Score with only selected features: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_train_selected, y_test)))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414204049970.png" alt="image-20220414204049970"></p>
<h4 id="4-5-2-基于模型的特征选择"><a href="#4-5-2-基于模型的特征选择" class="headerlink" title="4.5.2 基于模型的特征选择"></a>4.5.2 基于模型的特征选择</h4><p> 基于模型的特征选择使用一个监督机器学习模型来判断每个特征的重要性，并且仅保留最重要的特征。用于特征选择的模型不需要与用于最终监督建模的模型相同。与单变量选择不同，基于模型的选择同时考虑所有特征。</p>
<p>下面的例子使用包含100棵树的随机森林分类器来计算特征重要性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"><span class="comment"># 获得确定性的随机数</span></span><br><span class="line">rng = np.random.RandomState(<span class="number">42</span>)</span><br><span class="line">noise = rng.normal(size=(<span class="built_in">len</span>(cancer.data), <span class="number">50</span>))</span><br><span class="line"><span class="comment"># 添加噪声特征</span></span><br><span class="line"><span class="comment"># 前30个来自数据集，后50个是噪声</span></span><br><span class="line">X_w_noise = np.hstack([cancer.data, noise])</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_w_noise, cancer.target, random_state=<span class="number">0</span>, test_size=<span class="number">.5</span>)</span><br><span class="line"><span class="comment"># 采用中位数作为阈值，这样可以得到一半的特征</span></span><br><span class="line">select = SelectFromModel(RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>),threshold=<span class="string">&quot;median&quot;</span>)</span><br><span class="line">select.fit(X_train, y_train)</span><br><span class="line">X_train_l1 = select.transform(X_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train_l1.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train_l1.shape))</span><br><span class="line">mask = select.get_support()</span><br><span class="line"><span class="comment"># 将遮罩可视化，黑色为被选中，白色为舍弃</span></span><br><span class="line">plt.matshow(mask.reshape(<span class="number">1</span>,-<span class="number">1</span>), cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Sample index&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414204108072.png" alt="image-20220414204108072"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414204118389.png" alt="image-20220414204118389"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_test_l1 = select.transform(X_test)</span><br><span class="line">score = LogisticRegression(max_iter=<span class="number">5000</span>).fit(X_train_l1, y_train).score(X_test_l1, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(score))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414204126541.png" alt="image-20220414204126541"></p>
<h4 id="4-5-3-迭代特征选择"><a href="#4-5-3-迭代特征选择" class="headerlink" title="4.5.3 迭代特征选择"></a>4.5.3 迭代特征选择</h4><p>在迭代特征选择中，将会构建一系列模型，每个模型都使用不同数量的特征。有两种基本方法：</p>
<ul>
<li>开始时没有特征，然后逐个添加特征，直到满足某个终止条件。</li>
<li>从所有特征开始，然后逐个删除特征，直到满足某个终止条件。</li>
</ul>
<p>其中一种特殊方法是递归特征消除（RFE），它从所有特征开始构建模型，并根据模型舍弃最不重要的特征，然后使用除被舍弃特征之外的所有特征来构建一个新的模型，如此继续，直到剩下预设数量的特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line">select = RFE(RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>),n_features_to_select=<span class="number">40</span>)</span><br><span class="line">select.fit(X_train, y_train)</span><br><span class="line">mask = select.get_support()</span><br><span class="line">plt.matshow(mask.reshape(<span class="number">1</span>,-<span class="number">1</span>), cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Sample index&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414204137686.png" alt="image-20220414204137686"></p>
<p>对一个随机森林训练了40次，每训练每一次就删除一个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_train_rfe = select.transform(X_train)</span><br><span class="line">X_test_rfe = select.transform(X_test)</span><br><span class="line"></span><br><span class="line">score = LogisticRegression(max_iter=<span class="number">1000</span>).fit(X_train_rfe, y_train).score(X_test_rfe, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(score))</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://kisugitakumi.oss-cn-chengdu.aliyuncs.com/img12/image-20220414204145980.png" alt="image-20220414204145980"></p>
<h3 id="4-6-小结"><a href="#4-6-小结" class="headerlink" title="4.6 小结"></a>4.6 小结</h3><p>对于线性模型，可能会从分箱，添加多项式和交互项而生成的新特征中大大受益，对于更加复杂的非线性模型（例如随机森林和SVM），在无需显示扩展特征空间的前提下就可以学习更加复杂的任务。在实践中，所使用的特征（以及特征与方法之间的匹配）通常是使机器学习方法表现良好的重要因素。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:zenghongyi1@google.com">Kisugi Takumi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://kisugitakumi.github.io/2021/04/15/Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">https://kisugitakumi.github.io/2021/04/15/Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kisugitakumi.github.io" target="_blank">Kisugi Takumi</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/Python/">Python</a></div><div class="post_share"><div class="social-share" data-image="/linear-gradient(20deg,%20#0062be,%20#925696,%20#cc426e,%20#fb0347)" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/05/01/%E4%B8%80%E5%8F%A5%E8%AF%9D%E6%96%87%E6%B3%95/"><img class="prev-cover" src="/img/bunpo.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">日语语法总结</div></div></a></div><div class="next-post pull-right"><a href="/2021/03/20/Nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><img class="next-cover" src="/img/nginx.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Nginx学习笔记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/12/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="深度学习入门学习笔记"><img class="cover" src="/img/deeplearning.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-23</div><div class="title">深度学习入门学习笔记</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0"><span class="toc-text">Python机器学习基础教程学习笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%BC%95%E8%A8%80"><span class="toc-text">第一章 引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%A6%82%E5%BF%B5"><span class="toc-text">1.1 概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%92%8C%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D"><span class="toc-text">1.2 环境搭建和工具介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%EF%BC%9A%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB"><span class="toc-text">1.3 第一个应用：鸢尾花分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-1-%E5%88%9D%E8%AF%86%E6%95%B0%E6%8D%AE"><span class="toc-text">1.3.1 初识数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-2-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%92%8C%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE"><span class="toc-text">1.3.2 训练数据和测试数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-3-%E8%A7%82%E5%AF%9F%E6%95%B0%E6%8D%AE"><span class="toc-text">1.3.3 观察数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-4-%E6%9E%84%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%EF%BC%9Ak%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95"><span class="toc-text">1.3.4 构建第一个模型：k近邻算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-5-%E5%81%9A%E5%87%BA%E9%A2%84%E6%B5%8B"><span class="toc-text">1.3.5 做出预测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-6-%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B"><span class="toc-text">1.3.6 评估模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-7-%E5%B0%8F%E7%BB%93"><span class="toc-text">1.3.7 小结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">第二章 监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92"><span class="toc-text">2.1 分类与回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%B3%9B%E5%8C%96%EF%BC%8C%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-text">2.2 泛化，过拟合和欠拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-text">2.3 监督学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-1-%E4%B8%80%E4%BA%9B%E6%A0%B7%E6%9C%AC%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2.3.1 一些样本数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-2-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95"><span class="toc-text">2.3.2 k近邻算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-text">2.3.3  线性模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-4-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-text">2.3.4 朴素贝叶斯分类器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-5-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-text">2.3.5 决策树</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-6-%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90"><span class="toc-text">2.3.6 决策树集成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-7-%E6%A0%B8%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-text">2.3.7 核支持向量机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-8-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89"><span class="toc-text">2.3.8 神经网络（深度学习）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">第三章 无监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="toc-text">3.1 无监督学习的类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-text">3.2 无监督学习的挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E7%BC%A9%E6%94%BE"><span class="toc-text">3.3 预处理和缩放</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-1-%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">3.3.1 不同类型的预处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-%E5%BA%94%E7%94%A8%E6%95%B0%E6%8D%AE%E5%8F%98%E6%8D%A2"><span class="toc-text">3.3.2 应用数据变换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-3-%E5%AF%B9%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%92%8C%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E7%9B%B8%E5%90%8C%E7%9A%84%E7%BC%A9%E6%94%BE"><span class="toc-text">3.3.3 对训练数据和测试数据进行相同的缩放</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-4-%E9%A2%84%E5%A4%84%E7%90%86%E5%AF%B9%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-text">3.3.4 预处理对监督学习的作用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E9%99%8D%E7%BB%B4%EF%BC%8C%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0"><span class="toc-text">3.4 降维，特征提取与流形学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-1%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90PCA"><span class="toc-text">3.4.1主成分分析PCA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-2-%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3NMF"><span class="toc-text">3.4.2 非负矩阵分解NMF</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-3-%E7%94%A8t-SNE%E8%BF%9B%E8%A1%8C%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0"><span class="toc-text">3.4.3 用t-SNE进行流形学习</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E8%81%9A%E7%B1%BB"><span class="toc-text">3.5 聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-1-k%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB"><span class="toc-text">3.5.1 k均值聚类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-2-%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB"><span class="toc-text">3.5.2 凝聚聚类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-3-DBSCAN"><span class="toc-text">3.5.3 DBSCAN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-4-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E6%AF%94%E4%B8%8E%E8%AF%84%E4%BC%B0"><span class="toc-text">3.5.4 聚类算法的对比与评估</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-5-%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E5%B0%8F%E7%BB%93"><span class="toc-text">3.5.5 聚类方法小结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E6%95%B0%E6%8D%AE%E8%A1%A8%E7%A4%BA%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-text">第四章 数据表示与特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F"><span class="toc-text">4.1 分类变量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-One-Hot%E7%BC%96%E7%A0%81%EF%BC%88%E8%99%9A%E6%8B%9F%E5%8F%98%E9%87%8F%EF%BC%89"><span class="toc-text">4.1.1 One-Hot编码（虚拟变量）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-%E6%95%B0%E5%AD%97%E5%8F%AF%E4%BB%A5%E7%BC%96%E7%A0%81%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F"><span class="toc-text">4.1.2 数字可以编码分类变量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%88%86%E7%AE%B1%EF%BC%8C%E7%A6%BB%E6%95%A3%E5%8C%96%EF%BC%8C%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%A0%91"><span class="toc-text">4.2 分箱，离散化，线性模型与树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E4%BA%A4%E4%BA%92%E7%89%B9%E5%BE%81%E4%B8%8E%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%89%B9%E5%BE%81"><span class="toc-text">4.3 交互特征与多项式特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%8D%95%E5%8F%98%E9%87%8F%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2"><span class="toc-text">4.4 单变量非线性变换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E8%87%AA%E5%8A%A8%E5%8C%96%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-text">4.5 自动化特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-1-%E5%8D%95%E5%8F%98%E9%87%8F%E7%BB%9F%E8%AE%A1"><span class="toc-text">4.5.1 单变量统计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-2-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-text">4.5.2 基于模型的特征选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-3-%E8%BF%AD%E4%BB%A3%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-text">4.5.3 迭代特征选择</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E5%B0%8F%E7%BB%93"><span class="toc-text">4.6 小结</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: linear-gradient(20deg, #0062be, #925696, #cc426e, #fb0347)"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By Kisugi Takumi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my blog!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>